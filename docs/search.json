[
  {
    "objectID": "1_dataCleaning.html",
    "href": "1_dataCleaning.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "This notebook handles the data cleaning process for the datasets, documenting the process at the same time.\n# Libraries\nimport json\nimport pandas as pd\nimport numpy as np\nimport re\nfrom pathlib import Path"
  },
  {
    "objectID": "1_dataCleaning.html#raw-datasets",
    "href": "1_dataCleaning.html#raw-datasets",
    "title": "Data Cleaning",
    "section": "Raw Datasets",
    "text": "Raw Datasets\nThe raw datasets are stored in the data/raw directory. The datasets include:\n\nbautismos.csv: Baptism records\nmatrimonios.csv: Marriage records\nentierros.csv: Burial records\n\n\nBAUTISMOS_RAW = pd.read_csv(\"../data/raw/bautismos.csv\")\nMATRIMONIOS_RAW = pd.read_csv(\"../data/raw/matrimonios.csv\")\nENTIERROS_RAW = pd.read_csv(\"../data/raw/entierros.csv\")\n\nBAUTISMOS_RAW.head()\n\n\n\n\n\n\n\n\nSecuencia\nUnidad Documental Compuesta (a la que pertenece)\nIdentificador (es recomendable seguir una secuencia numeral como la mostrada en los ejemplos)\nTítulo (incluir un título breve para cada documento)\nFolio inicial del documento (convertir como se muestra abajo)\nFolio final del documento (convertir como se muestra abajo)\nImagen inicial (estos valores serán añadidos cuando comienze el proceso de revisión de imágenes)\nImagen final (estos valores serán añadidos cuando comienze el proceso de revisión de imágenes)\nTipo de evento\nFecha aaaa-mm-dd\n...\nCondición de la madrina\nLugar de bautizo\nNotas adicionales del documento\nDescriptor Geográfico 1\nDescriptor Geográfico 2\nDescriptor Geográfico 3\nDescriptor Geográfico 4\n5\nCaracterísticas físicas (Estado de conservación de los materiales físicos)\nHistoria de revisión (de los materiales digitalizados)\n\n\n\n\n0\n1.0\nAPAucará LB L001\nB001\nBautizo. Domingo. Tributarios\n3r\n3r\nIMG_7000a\nIMG_7000a\nBautizo\n1790-10-04\n...\nNaN\nPampamarca, iglesia\nNaN\nAucara\nPampamarca\nNaN\nNaN\nNaN\nRegular\nRegistrado por Edwin Gonzales en 2023\n\n\n1\n2.0\nAPAucará LB L001\nB002\nBautizo. Dominga. Tributarios\n3r\n3r\nIMG_7000a\nIMG_7000a\nBautizo\n1790-10-06\n...\nNaN\nPampamarca, iglesia\nNaN\nAucara\nPampamarca\nNaN\nNaN\nNaN\nRegular\nRegistrado por Edwin Gonzales en 2023\n\n\n2\n3.0\nAPAucará LB L001\nB003\nBautizo. Bartola. Tributarios\n3r\n3r\nIMG_7000a\nIMG_7000a\nBautizo\n1790-10-07\n...\nNaN\nPampamarca, iglesia\nNaN\nAucara\nPampamarca\nNaN\nNaN\nNaN\nRegular\nRegistrado por Edwin Gonzales en 2023\n\n\n3\n4.0\nAPAucará LB L001\nB004\nBautizo. Francisca\n3v\n3v\nIMG_7000b\nIMG_7000b\nBautizo\n1790-10-20\n...\nNaN\nAucara, iglesia\nAbreviatura poco visible en el margen\nAucara\nNaN\nNaN\nNaN\nNaN\nRegular\nRegistrado por Edwin Gonzales en 2023\n\n\n4\n5.0\nAPAucará LB L001\nB005\nBautizo. Pedro\n3v\n3v\nIMG_7000b\nIMG_7000b\nBautizo\n1790-10-20\n...\nNaN\nAucara, iglesia\nMargen roto y manchado de tinta\nAucara\nNaN\nNaN\nNaN\nNaN\nRegular\nRegistrado por Edwin Gonzales en 2023\n\n\n\n\n5 rows × 36 columns"
  },
  {
    "objectID": "1_dataCleaning.html#column-harmonization",
    "href": "1_dataCleaning.html#column-harmonization",
    "title": "Data Cleaning",
    "section": "Column Harmonization",
    "text": "Column Harmonization\nRename the columns in the datasets to ensure consistency across different data sources.\nValues are mapped using the json files located in the data/mappings directory.\n\nfrom utils.ColumnManager import ColumnManager\n\n\nbautismoMapping = Path(\"../data/mappings/bautismosMapping.json\")\nmatrimonioMapping = Path(\"../data/mappings/matrimoniosMapping.json\")\nentierroMapping = Path(\"../data/mappings/entierrosMapping.json\")\n\ncolumn_manager = ColumnManager()\n\nBAUTISMOS_HARMONIZED = column_manager.harmonize_columns(BAUTISMOS_RAW, bautismoMapping)\nMATRIMONIOS_HARMONIZED = column_manager.harmonize_columns(MATRIMONIOS_RAW, matrimonioMapping)\nENTIERROS_HARMONIZED = column_manager.harmonize_columns(ENTIERROS_RAW, entierroMapping)\n\nBAUTISMOS_HARMONIZED.head()\n\n\n\n\n\n\n\n\nid\nfile\nidentifier\ntitle\nstart_folio\nend_folio\nstart_image\nend_image\nevent_type\nevent_date\n...\ngodmother_social_condition\nevent_place\nevent_additional_notes\nevent_geographic_descriptor_1\nevent_geographic_descriptor_2\nevent_geographic_descriptor_3\nevent_geographic_descriptor_4\nevent_other\nrecord_physical_characteristics\nrevision_history\n\n\n\n\n0\n1.0\nAPAucará LB L001\nB001\nBautizo. Domingo. Tributarios\n3r\n3r\nIMG_7000a\nIMG_7000a\nBautizo\n1790-10-04\n...\nNaN\nPampamarca, iglesia\nNaN\nAucara\nPampamarca\nNaN\nNaN\nNaN\nRegular\nRegistrado por Edwin Gonzales en 2023\n\n\n1\n2.0\nAPAucará LB L001\nB002\nBautizo. Dominga. Tributarios\n3r\n3r\nIMG_7000a\nIMG_7000a\nBautizo\n1790-10-06\n...\nNaN\nPampamarca, iglesia\nNaN\nAucara\nPampamarca\nNaN\nNaN\nNaN\nRegular\nRegistrado por Edwin Gonzales en 2023\n\n\n2\n3.0\nAPAucará LB L001\nB003\nBautizo. Bartola. Tributarios\n3r\n3r\nIMG_7000a\nIMG_7000a\nBautizo\n1790-10-07\n...\nNaN\nPampamarca, iglesia\nNaN\nAucara\nPampamarca\nNaN\nNaN\nNaN\nRegular\nRegistrado por Edwin Gonzales en 2023\n\n\n3\n4.0\nAPAucará LB L001\nB004\nBautizo. Francisca\n3v\n3v\nIMG_7000b\nIMG_7000b\nBautizo\n1790-10-20\n...\nNaN\nAucara, iglesia\nAbreviatura poco visible en el margen\nAucara\nNaN\nNaN\nNaN\nNaN\nRegular\nRegistrado por Edwin Gonzales en 2023\n\n\n4\n5.0\nAPAucará LB L001\nB005\nBautizo. Pedro\n3v\n3v\nIMG_7000b\nIMG_7000b\nBautizo\n1790-10-20\n...\nNaN\nAucara, iglesia\nMargen roto y manchado de tinta\nAucara\nNaN\nNaN\nNaN\nNaN\nRegular\nRegistrado por Edwin Gonzales en 2023\n\n\n\n\n5 rows × 36 columns\n\n\n\n\nReduce DataFrames to their relevant columns\n\nuseful_columns = json.load(open(\"../data/mappings/usefulColumnsMapping.json\"))\n\n\n# Bautismos useful columns\n\nBAUTISMOS_HARMONIZED = BAUTISMOS_HARMONIZED[useful_columns['bautizo']]\n\n# remove empty columns\nBAUTISMOS_HARMONIZED.dropna(axis=1, how='all', inplace=True)\n\nBAUTISMOS_HARMONIZED.columns\n\nIndex(['file', 'identifier', 'event_type', 'event_date', 'baptized_name',\n       'baptized_hometown', 'baptized_birth_date',\n       'baptized_legitimacy_status', 'father_name', 'father_lastname',\n       'father_social_condition', 'mother_name', 'mother_lastname',\n       'mother_social_condition', 'parents_social_condition', 'godfather_name',\n       'godfather_lastname', 'godfather_social_condition', 'godmother_name',\n       'godmother_lastname', 'godmother_social_condition', 'event_place',\n       'event_geographic_descriptor_1', 'event_geographic_descriptor_2',\n       'event_geographic_descriptor_3', 'event_geographic_descriptor_4'],\n      dtype='object')\n\n\n\nMATRIMONIOS_HARMONIZED = MATRIMONIOS_HARMONIZED[useful_columns['matrimonio']]\n\n# remove empty columns\nMATRIMONIOS_HARMONIZED.dropna(axis=1, how='all', inplace=True)\n\nMATRIMONIOS_HARMONIZED.columns\n\nIndex(['file', 'identifier', 'event_type', 'event_date', 'groom_name',\n       'groom_lastname', 'groom_social_condition', 'groom_marital_status',\n       'groom_age', 'groom_hometown', 'groom_resident_in',\n       'groom_legitimacy_status', 'groom_father_name', 'groom_father_lastname',\n       'groom_father_social_condition', 'groom_mother_name',\n       'groom_mother_lastname', 'groom_mother_social_condition', 'bride_name',\n       'bride_lastname', 'bride_social_condition', 'bride_marital_status',\n       'bride_age', 'bride_hometown', 'bride_resident_in',\n       'bride_legitimacy_status', 'bride_father_name', 'bride_father_lastname',\n       'bride_father_social_condition', 'bride_mother_name',\n       'bride_mother_lastname', 'bride_mother_social_condition',\n       'godparent_1_name', 'godparent_1_lastname',\n       'godparent_1_social_condition', 'godparent_2_name',\n       'godparent_2_lastname', 'godparent_2_social_condition',\n       'godparent_3_name', 'godparent_3_lastname', 'witness_1_name',\n       'witness_1_lastname', 'witness_2_name', 'witness_2_lastname',\n       'witness_3_name', 'witness_3_lastname', 'witness_4_name',\n       'witness_4_lastname', 'event_place', 'event_geographic_descriptor_1',\n       'event_geographic_descriptor_2', 'event_geographic_descriptor_3',\n       'event_geographic_descriptor_4', 'event_geographic_descriptor_5',\n       'event_geographic_descriptor_6'],\n      dtype='object')\n\n\n\nENTIERROS_HARMONIZED = ENTIERROS_HARMONIZED[useful_columns['entierro']]\n\n# remove empty columns\nENTIERROS_HARMONIZED.dropna(axis=1, how='all', inplace=True)\n\nENTIERROS_HARMONIZED.columns\n\nIndex(['file', 'identifier', 'event_type', 'event_date', 'doctrine',\n       'event_place', 'deceased_name', 'deceased_lastname', 'deceased_age',\n       'deceased_hometown', 'deceased_condition', 'deceased_marital_status',\n       'deceased_legitimacy_status', 'father_name', 'father_lastname',\n       'mother_name', 'mother_lastname', 'husband_name', 'wife_name',\n       'burial_place', 'event_geographic_descriptor_1',\n       'event_geographic_descriptor_2', 'event_geographic_descriptor_3',\n       'event_geographic_descriptor_4'],\n      dtype='object')"
  },
  {
    "objectID": "1_dataCleaning.html#replace-empty-or-null-values-with-na",
    "href": "1_dataCleaning.html#replace-empty-or-null-values-with-na",
    "title": "Data Cleaning",
    "section": "Replace empty or null values with ‘na’",
    "text": "Replace empty or null values with ‘na’\n\n## replace cells with no textual information with numpy na\ndef replace_empty_with_na(df):\n    \"\"\"\n    Replace placeholder strings with np.nan in string columns only.\n    \"\"\"\n    placeholders = {'', '-', '--', 'n/a', 'na', 'null', 'None'}\n\n    def clean_cell(val):\n        if isinstance(val, str) and val.strip().lower() in placeholders:\n            return np.nan\n        return val\n\n    return df.map(clean_cell)\n\n\nBAUTISMOS_HARMONIZED = replace_empty_with_na(BAUTISMOS_HARMONIZED)\nMATRIMONIOS_HARMONIZED = replace_empty_with_na(MATRIMONIOS_HARMONIZED)\nENTIERROS_HARMONIZED = replace_empty_with_na(ENTIERROS_HARMONIZED)"
  },
  {
    "objectID": "1_dataCleaning.html#dates-normalization",
    "href": "1_dataCleaning.html#dates-normalization",
    "title": "Data Cleaning",
    "section": "Dates Normalization",
    "text": "Dates Normalization\nEnsure dates are in a consistent format across all datasets. The dates should be in the format YYYY-MM-DD.\n\nfrom actions.normalizers.DatesNormalizer import DateNormalizer\n\n\nBAUTISMOS_HARMONIZED['event_date'] = DateNormalizer(BAUTISMOS_HARMONIZED['event_date']).normalize()\nBAUTISMOS_HARMONIZED['event_date']\n\n0       1790-10-04\n1       1790-10-06\n2       1790-10-07\n3       1790-10-20\n4       1790-10-20\n           ...    \n6336    1888-12-10\n6337    1888-12-11\n6338    1888-12-12\n6339    1888-12-15\n6340    1888-12-16\nName: event_date, Length: 6341, dtype: object\n\n\n\nMATRIMONIOS_HARMONIZED['event_date'] = DateNormalizer(MATRIMONIOS_HARMONIZED['event_date']).normalize()\nMATRIMONIOS_HARMONIZED['event_date']\n\n0       1816-12-06\n1       1816-12-12\n2       1817-03-05\n3       1817-03-10\n4       1817-03-12\n           ...    \n1714    1907-10-27\n1715    1908-01-13\n1716    1908-01-15\n1717    1908-02-15\n1718    1908-03-17\nName: event_date, Length: 1719, dtype: object\n\n\n\nENTIERROS_HARMONIZED['event_date'] = DateNormalizer(ENTIERROS_HARMONIZED['event_date']).normalize()\nENTIERROS_HARMONIZED['event_date']\n\n0       1846-10-06\n1       1846-10-07\n2       1846-11-02\n3       1846-12-08\n4       1847-02-23\n           ...    \n2193    1920-10-12\n2194    1920-10-19\n2195    1920-10-19\n2196    1920-10-20\n2197    1920-10-21\nName: event_date, Length: 2198, dtype: object"
  },
  {
    "objectID": "1_dataCleaning.html#age-inferring",
    "href": "1_dataCleaning.html#age-inferring",
    "title": "Data Cleaning",
    "section": "Age Inferring",
    "text": "Age Inferring\nInfer the age of individuals based on their birth dates and the date of the event (baptism, marriage, burial).\n\nfrom actions.generators.AgeInferrer import AgeInferrer\n\n\nBAUTISMOS_HARMONIZED['baptized_birth_date'] = AgeInferrer(BAUTISMOS_HARMONIZED['event_date']).infer_all(BAUTISMOS_HARMONIZED['baptized_birth_date'])\nBAUTISMOS_HARMONIZED[['event_date', 'baptized_birth_date']]\n\n\n\n\n\n\n\n\nevent_date\nbaptized_birth_date\n\n\n\n\n0\n1790-10-04\n1790-08-04\n\n\n1\n1790-10-06\n1790-08-04\n\n\n2\n1790-10-07\n1790-08-04\n\n\n3\n1790-10-20\n1790-10-15\n\n\n4\n1790-10-20\n1790-10-19\n\n\n...\n...\n...\n\n\n6336\n1888-12-10\n1888-12-09\n\n\n6337\n1888-12-11\n1888-12-07\n\n\n6338\n1888-12-12\n1888-12-06\n\n\n6339\n1888-12-15\n1888-11-30\n\n\n6340\n1888-12-16\n1888-12-01\n\n\n\n\n6341 rows × 2 columns\n\n\n\n\n# Inconsistent dates: event_date should be after baptized_birth_date\ninvalid_mask = pd.to_datetime(BAUTISMOS_HARMONIZED['event_date'], errors='coerce') &lt; pd.to_datetime(BAUTISMOS_HARMONIZED['baptized_birth_date'], errors='coerce')\nif invalid_mask.any():\n    print(\"Found invalid records:\")\n    print(BAUTISMOS_HARMONIZED[invalid_mask][['event_date', 'baptized_birth_date']])\n\nFound invalid records:\n      event_date baptized_birth_date\n135   1792-03-29          1792-04-08\n290   1794-01-01          1794-01-27\n671   1797-07-15          1797-07-24\n2814  1900-04-01          1900-04-09\n\n\n\nUnfortunately, these records were incorrectly recorded. To fix this, it is necessary to check with the original records.\n\n\nMATRIMONIOS_HARMONIZED['groom_age'] = AgeInferrer(MATRIMONIOS_HARMONIZED['event_date']).infer_all(MATRIMONIOS_HARMONIZED['groom_age'])\nMATRIMONIOS_HARMONIZED['bride_age'] = AgeInferrer(MATRIMONIOS_HARMONIZED['event_date']).infer_all(MATRIMONIOS_HARMONIZED['bride_age'])\nMATRIMONIOS_HARMONIZED[['event_date', 'groom_age', 'bride_age']]\n\n\n\n\n\n\n\n\nevent_date\ngroom_age\nbride_age\n\n\n\n\n0\n1816-12-06\nNaN\nNaN\n\n\n1\n1816-12-12\nNaN\nNaN\n\n\n2\n1817-03-05\nNaN\nNaN\n\n\n3\n1817-03-10\nNaN\nNaN\n\n\n4\n1817-03-12\nNaN\nNaN\n\n\n...\n...\n...\n...\n\n\n1714\n1907-10-27\n1882-11-01\n1880-11-01\n\n\n1715\n1908-01-13\n1880-01-19\n1879-01-19\n\n\n1716\n1908-01-15\n1886-01-19\n1888-01-19\n\n\n1717\n1908-02-15\n1883-02-20\n1886-02-19\n\n\n1718\n1908-03-17\n1880-03-23\n1879-03-24\n\n\n\n\n1719 rows × 3 columns\n\n\n\n\nENTIERROS_HARMONIZED['deceased_age'] = AgeInferrer(ENTIERROS_HARMONIZED['event_date']).infer_all(ENTIERROS_HARMONIZED['deceased_age'])\nENTIERROS_HARMONIZED[['event_date', 'deceased_age']]\n\n\n\n\n\n\n\n\nevent_date\ndeceased_age\n\n\n\n\n0\n1846-10-06\nNaN\n\n\n1\n1846-10-07\n1821-10-13\n\n\n2\n1846-11-02\n1766-11-21\n\n\n3\n1846-12-08\n1806-12-18\n\n\n4\n1847-02-23\n1797-03-06\n\n\n...\n...\n...\n\n\n2193\n1920-10-12\nNone\n\n\n2194\n1920-10-19\nNone\n\n\n2195\n1920-10-19\nNone\n\n\n2196\n1920-10-20\nNone\n\n\n2197\n1920-10-21\nNone\n\n\n\n\n2198 rows × 2 columns"
  },
  {
    "objectID": "1_dataCleaning.html#names-normalization",
    "href": "1_dataCleaning.html#names-normalization",
    "title": "Data Cleaning",
    "section": "Names Normalization",
    "text": "Names Normalization\nStandardize names across datasets to ensure consistency. This includes normalizing first names, last names, and any other relevant name fields.\n\nfrom actions.normalizers.NamesNormalizer import NamesNormalizer\n\n\ndef normalize_names_columns(series):\n    namesManager = NamesNormalizer()\n    return namesManager.clean_series(series)\n\n\nnames_columns = [\n    'baptized_name', \n    'father_name', 'father_lastname',\n    'mother_name', 'mother_lastname',\n    'godfather_name', 'godfather_lastname', \n    'godmother_name', 'godmother_lastname',\n]\n\nfor col in names_columns:\n    if col in BAUTISMOS_HARMONIZED.columns:\n        BAUTISMOS_HARMONIZED[col] = normalize_names_columns(BAUTISMOS_HARMONIZED[col])\n\nBAUTISMOS_HARMONIZED[names_columns]\n\n\n\n\n\n\n\n\nbaptized_name\nfather_name\nfather_lastname\nmother_name\nmother_lastname\ngodfather_name\ngodfather_lastname\ngodmother_name\ngodmother_lastname\n\n\n\n\n0\ndomingo\nlucas\nayquipa\nsevastiana\nquispe\nvicente\nguamani\nNaN\nNaN\n\n\n1\ndominga\njuan\nlulia\njospha\ngomes\nignacio\nvarientos\nNaN\nNaN\n\n\n2\nbartola\njacinto\nquispe\njuliana\nchinchay\nNaN\nNaN\nrotonda\npocco\n\n\n3\nfrancisca\njuan\ncuebas\nclemenzia\nmanco\nNaN\nNaN\nysabel\nguillen\n\n\n4\npedro\nsantos\nmanxo\nbaleriana\narango\nNaN\nNaN\njosefa\nsantiago\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n6336\nleocadio\nmiguel\npacheco\nrosa\nhuarcaya\njosé julián\nbendezú\nNaN\nNaN\n\n\n6337\nmariano concepcion\nfacundo\nvega\nsilvestra\nurbano\nfernando\nmancco\nNaN\nNaN\n\n\n6338\nambrosio\nysidro\nccasane\nrita\npalomino\njuan\ntito\nNaN\nNaN\n\n\n6339\nfrancisco\nmariano\nlopez\nleocadia\nmedina\nfeliciano\ndias\nNaN\nNaN\n\n\n6340\nlaureana\nbernarda\nchampa\nrufina\nlopez\nNaN\nNaN\nmanuela\nde la cruz\n\n\n\n\n6341 rows × 9 columns\n\n\n\n\nmatrimonios_names_columns = [\n    'groom_name', 'groom_lastname', \n       'groom_father_name', 'groom_father_lastname', \n       'groom_mother_name', 'groom_mother_lastname',\n       'bride_name', 'bride_lastname',\n       'bride_father_name', 'bride_father_lastname',\n       'bride_mother_name', 'bride_mother_lastname', \n       'godparent_1_name', 'godparent_1_lastname',\n       'godparent_2_name', 'godparent_2_lastname', \n       'godparent_3_name', 'godparent_3_lastname', \n       'witness_1_name', 'witness_1_lastname', \n       'witness_2_name', 'witness_2_lastname',\n       'witness_3_name', 'witness_3_lastname', \n       'witness_4_name', 'witness_4_lastname'\n]\n\nfor col in matrimonios_names_columns:\n    if col in MATRIMONIOS_HARMONIZED.columns:\n        MATRIMONIOS_HARMONIZED[col] = normalize_names_columns(MATRIMONIOS_HARMONIZED[col])\n\nMATRIMONIOS_HARMONIZED[matrimonios_names_columns]\n\n\n\n\n\n\n\n\ngroom_name\ngroom_lastname\ngroom_father_name\ngroom_father_lastname\ngroom_mother_name\ngroom_mother_lastname\nbride_name\nbride_lastname\nbride_father_name\nbride_father_lastname\n...\ngodparent_3_name\ngodparent_3_lastname\nwitness_1_name\nwitness_1_lastname\nwitness_2_name\nwitness_2_lastname\nwitness_3_name\nwitness_3_lastname\nwitness_4_name\nwitness_4_lastname\n\n\n\n\n0\njosé manl manuel\nde la roca\nacencio\nroca\nleonor\nguerrero\njuana\nrodrigues\npedro\nrodrigues\n...\nNaN\nNaN\nagustin\ncastro\nmariano\ncastro\njuan\nbaldes\nNaN\nNaN\n\n\n1\nesteban\ncastillo\nmatheo\ncastillo\nma maria\ntorres\nambrocia\ntasqui\npedro\ntasqui\n...\nNaN\nNaN\npedro\nmanco\ncarlos\ncanto\npedro\nguamani\nNaN\nNaN\n\n\n2\nalexandro\nramires\nleonor\nromani\nfranca francisca\npaucar\nsipriana\ncoillo\ncristobal\ncoillo\n...\nNaN\nNaN\nmarcelo\nllamuca\njulian\nurbano\nantonio\nurbano\nNaN\nNaN\n\n\n3\njose\ncuchu\nacencio\ncuchu\nbaleriana\nantay\ncacimira\nflores\nNaN\nNaN\n...\nNaN\nNaN\npablo\nroque\nantonio\nurbano\ncristobal\ncoillo\nNaN\nNaN\n\n\n4\ndomingo\ntito\nNaN\nNaN\nmarcela\nguauya\npetrona\nguallpatuiru\nagustin\nguallpatuiru\n...\nNaN\nNaN\nmarcelo\nllamuca\nantonio\nguamani\nmariano\nguallpatuiru\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1714\npatrocinio\nchinchay\nmiguel\nchinchay\nandrea\npolanco\nlorenza\nquispe\ngervacio\nquispe\n...\nNaN\nNaN\njuan de dios\nbarrientos\nmanuel\nespinosa\ncrisostomo\npumarino\nNaN\nNaN\n\n\n1715\ngerónimo\ncucho\nambrocio\ncucho\ngertrudis\nserrano\nteresa\njimenes\naniseto\njimenes\n...\nNaN\nNaN\nvictor\nsaravia\nmateo\naiquipa\nfelix\ncucho\nNaN\nNaN\n\n\n1716\njosé\ncoro\nfelix\ncoro\nnatividad\ncucho\nemilia\nhuamani\npatricio\nhuamani\n...\nNaN\nNaN\npablo\nde la cruz\nvictor\nsaravia\nmarcelo\nramos\nNaN\nNaN\n\n\n1717\npedro\ngutierres\nruperto\ngutierrez\nmicaila\noscco\njuliana\nhuarcaya\nhilario\nhuarcaya\n...\nNaN\nNaN\nrafael\ndelgado\njosé\nvivanco\nagustin\nvicente\nNaN\nNaN\n\n\n1718\nbenito\nsanchez\ntiburcio\nsanchez\nlorenza\naymi\nteresa\npoma\nbernardo\npoma\n...\nNaN\nNaN\nsantos\nespejo\nfelix\nsanches\nmariano\nurbano\nNaN\nNaN\n\n\n\n\n1719 rows × 26 columns\n\n\n\n\nentierros_names_columns = [\n    \"deceased_name\", \"deceased_lastname\",\n    \"father_name\", \"father_lastname\",\n    \"mother_name\", \"mother_lastname\",\n    \"husband_name\", \"wife_name\",\n]\n\nfor col in entierros_names_columns:\n    if col in ENTIERROS_HARMONIZED.columns:\n        ENTIERROS_HARMONIZED[col] = normalize_names_columns(ENTIERROS_HARMONIZED[col])\n\nENTIERROS_HARMONIZED[entierros_names_columns]\n\n\n\n\n\n\n\n\ndeceased_name\ndeceased_lastname\nfather_name\nfather_lastname\nmother_name\nmother_lastname\nhusband_name\nwife_name\n\n\n\n\n0\njulian\nxavies\nNaN\nNaN\nNaN\nNaN\nNaN\nmercedes lupa\n\n\n1\njoce\nraime\nNaN\nNaN\nNaN\nNaN\nNaN\nfrancisca cucho\n\n\n2\nmartina\ncondori\nNaN\nNaN\nNaN\nNaN\nluciano ccoyllo\nNaN\n\n\n3\ndorotea\nccoyllo\nNaN\nNaN\nNaN\nNaN\njosé espinosa\nNaN\n\n\n4\nmaría\nromani\nNaN\nNaN\nNaN\nNaN\nmariano huallpatuiro\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2193\nnieves\nhuallpatuero\npatrocinio\nhuallpatuero\nteresa\nurbano\nNaN\nNaN\n\n\n2194\nsinforiano\nhuamani\neustaquio\nhuamani\nmartina\nllamoca\nNaN\ngregoria ccoillo\n\n\n2195\nsalomé\ncondori\nsebastian\ncondori\nanacla\nroque\nNaN\nNaN\n\n\n2196\nmaría\nchinchay\nabdon\nchinchay\nfloriza\nlopez\nNaN\nNaN\n\n\n2197\nfortunato\nflorez\ncecilio\nflorez\nguadalupe\nramos\nNaN\nNaN\n\n\n\n\n2198 rows × 8 columns"
  },
  {
    "objectID": "1_dataCleaning.html#place-recognition",
    "href": "1_dataCleaning.html#place-recognition",
    "title": "Data Cleaning",
    "section": "Place Recognition",
    "text": "Place Recognition\nThis process involves NER recognition to extract places entities from the text fields in the datasets.\n\nfrom actions.extractors import placeRecognition\n\nextractor = placeRecognition.PlaceExtractor()\n\n\nbautismos_place_columns = [\n    'baptized_hometown', 'event_place', 'event_geographic_descriptor_1',\n        'event_geographic_descriptor_2', 'event_geographic_descriptor_3',\n        'event_geographic_descriptor_4'\n]\n\nBAUTISMOS_PLACES_RAW = BAUTISMOS_HARMONIZED[bautismos_place_columns]\n\nfor col in bautismos_place_columns:\n    if col in BAUTISMOS_HARMONIZED.columns:\n        BAUTISMOS_HARMONIZED[col] = extractor.extract_places_per_row(BAUTISMOS_HARMONIZED[col])\n\nBAUTISMOS_HARMONIZED[bautismos_place_columns]\n\n\n\n\n\n\n\n\nbaptized_hometown\nevent_place\nevent_geographic_descriptor_1\nevent_geographic_descriptor_2\nevent_geographic_descriptor_3\nevent_geographic_descriptor_4\n\n\n\n\n0\nNaN\nPampamarca\nNaN\nPampamarca\nNaN\nNaN\n\n\n1\nNaN\nPampamarca\nNaN\nPampamarca\nNaN\nNaN\n\n\n2\nNaN\nPampamarca\nNaN\nPampamarca\nNaN\nNaN\n\n\n3\nNaN\nAucara\nNaN\nNaN\nNaN\nNaN\n\n\n4\nNaN\nAucara\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n6336\nNaN\nAucará\nNaN\nNaN\nNaN\nNaN\n\n\n6337\nNaN\nAucará\nNaN\nNaN\nNaN\nNaN\n\n\n6338\nNaN\nAucará\nNaN\nMayobamba\nNaN\nNaN\n\n\n6339\nNaN\nAucará\nNaN\nHuaicahuacho\nNaN\nNaN\n\n\n6340\nNaN\nAucará\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n6341 rows × 6 columns\n\n\n\n\nmatrimonios_place_columns = [\n    'groom_hometown',\n       'groom_resident_in', \n       'bride_hometown', 'bride_resident_in', \n       'event_place', 'event_geographic_descriptor_1', 'event_geographic_descriptor_2',\n       'event_geographic_descriptor_3', 'event_geographic_descriptor_4',\n       'event_geographic_descriptor_5', 'event_geographic_descriptor_6'\n]\n\nMATRIMONIOS_PLACES_RAW = MATRIMONIOS_HARMONIZED[matrimonios_place_columns]\n\nfor col in matrimonios_place_columns:\n    if col in MATRIMONIOS_HARMONIZED.columns:\n        MATRIMONIOS_HARMONIZED[col] = extractor.extract_places_per_row(MATRIMONIOS_HARMONIZED[col])\n\nMATRIMONIOS_HARMONIZED[matrimonios_place_columns]\n\n\n\n\n\n\n\n\ngroom_hometown\ngroom_resident_in\nbride_hometown\nbride_resident_in\nevent_place\nevent_geographic_descriptor_1\nevent_geographic_descriptor_2\nevent_geographic_descriptor_3\nevent_geographic_descriptor_4\nevent_geographic_descriptor_5\nevent_geographic_descriptor_6\n\n\n\n\n0\nCiudad de Huamanga\nAucara\nNaN\nNaN\nAucara\nNaN\nHuamanga\nNaN\nNaN\nNaN\nNaN\n\n\n1\nNaN\nNaN\nNaN\nNaN\nAucara\nNaN\nColca\nNaN\nNaN\nNaN\nNaN\n\n\n2\nPampamarca\nNaN\nPampamarca\nNaN\nAucara\nNaN\nPampamarca\nNaN\nNaN\nNaN\nNaN\n\n\n3\nPampamarca\nNaN\nPampamarca\nNaN\nPampamarca|santa iglesia\nNaN\nPampamarca\nNaN\nNaN\nNaN\nNaN\n\n\n4\nNaN\nNaN\nNaN\nNaN\nPampamarca|santa iglesia\nNaN\nPampamarca\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1714\nPampamarca\nNaN\nPampamarca\nNaN\nPampamarca\nNaN\nPampamarca\nNaN\nNaN\nNaN\nNaN\n\n\n1715\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1716\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1717\nNaN\nNaN\nNaN\nNaN\nAucara\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1718\nPampamarca\nNaN\nPampamarca\nNaN\nPampamarca\nNaN\nPampamarca\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n1719 rows × 11 columns\n\n\n\n\nentierros_place_columns = [\n    'event_place', 'deceased_hometown', 'burial_place', 'event_geographic_descriptor_1',\n    'event_geographic_descriptor_2', 'event_geographic_descriptor_3',\n    'event_geographic_descriptor_4'\n]\n\nENTIERROS_PLACES_RAW = ENTIERROS_HARMONIZED[entierros_place_columns]\n\nfor col in entierros_place_columns:\n    if col in ENTIERROS_HARMONIZED.columns:\n        ENTIERROS_HARMONIZED[col] = extractor.extract_places_per_row(ENTIERROS_HARMONIZED[col])\n\nENTIERROS_HARMONIZED[entierros_place_columns]\n\n\n\n\n\n\n\n\nevent_place\ndeceased_hometown\nburial_place\nevent_geographic_descriptor_1\nevent_geographic_descriptor_2\nevent_geographic_descriptor_3\nevent_geographic_descriptor_4\n\n\n\n\n0\nNaN\nNaN\nNaN\nNaN\nLucanas\nNaN\nNaN\n\n\n1\nNaN\nNaN\nNaN\nNaN\nLucanas\nNaN\nNaN\n\n\n2\nNaN\nNaN\nNaN\nNaN\nLucanas\nNaN\nNaN\n\n\n3\nNaN\nNaN\nNaN\nNaN\nLucanas\nNaN\nNaN\n\n\n4\nNaN\nNaN\nNaN\nNaN\nLucanas\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2193\nAucara\nSanta Ana de Aucara\nNaN\nNaN\nSanta Ana de Aucara\nNaN\nNaN\n\n\n2194\nAucara\nPampamarca\nNaN\nNaN\nPampamarca\nNaN\nNaN\n\n\n2195\nAucara\nSanta Ana de Aucara\nNaN\nNaN\nSanta Ana de Aucara\nNaN\nNaN\n\n\n2196\nAucara\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2197\nAucara\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n2198 rows × 7 columns\n\n\n\n\n# save raw places\n\nfrom utils import UniqueValues\n\nuniqueValues = UniqueValues.UniqueValuesExtractor(\n    [BAUTISMOS_PLACES_RAW, MATRIMONIOS_PLACES_RAW, ENTIERROS_PLACES_RAW]\n).get_unique_values(return_dataframe=True)\n\nuniqueValues.to_csv(\"../data/raw/raw_places.csv\", index=False) # type: ignore #"
  },
  {
    "objectID": "1_dataCleaning.html#cleaning-audit",
    "href": "1_dataCleaning.html#cleaning-audit",
    "title": "Data Cleaning",
    "section": "Cleaning Audit",
    "text": "Cleaning Audit\nBasic audit to ensure that the data cleaning process has been successful."
  },
  {
    "objectID": "1_dataCleaning.html#save-cleaned-data",
    "href": "1_dataCleaning.html#save-cleaned-data",
    "title": "Data Cleaning",
    "section": "Save Cleaned Data",
    "text": "Save Cleaned Data\n\nclean_data_folder = Path(\"../data/clean/\")\n\n# Fill NaN values for consistency\nBAUTISMOS_HARMONIZED = BAUTISMOS_HARMONIZED.fillna(value=np.nan)\nMATRIMONIOS_HARMONIZED = MATRIMONIOS_HARMONIZED.fillna(value=np.nan)\nENTIERROS_HARMONIZED = ENTIERROS_HARMONIZED.fillna(value=np.nan)\n\nBAUTISMOS_HARMONIZED.to_csv(clean_data_folder / \"bautismos_clean.csv\", index=False)\nMATRIMONIOS_HARMONIZED.to_csv(clean_data_folder / \"matrimonios_clean.csv\", index=False)\nENTIERROS_HARMONIZED.to_csv(clean_data_folder / \"entierros_clean.csv\", index=False)\n\n\nBAUTISMOS_HARMONIZED.head()\n\n\n\n\n\n\n\n\nfile\nidentifier\nevent_type\nevent_date\nbaptized_name\nbaptized_hometown\nbaptized_birth_date\nbaptized_legitimacy_status\nfather_name\nfather_lastname\n...\ngodfather_lastname\ngodfather_social_condition\ngodmother_name\ngodmother_lastname\ngodmother_social_condition\nevent_place\nevent_geographic_descriptor_1\nevent_geographic_descriptor_2\nevent_geographic_descriptor_3\nevent_geographic_descriptor_4\n\n\n\n\n0\nAPAucará LB L001\nB001\nBautizo\n1790-10-04\ndomingo\nNaN\n1790-08-04\nHijo legitimo\nlucas\nayquipa\n...\nguamani\nNaN\nNaN\nNaN\nNaN\nPampamarca\nNaN\nPampamarca\nNaN\nNaN\n\n\n1\nAPAucará LB L001\nB002\nBautizo\n1790-10-06\ndominga\nNaN\n1790-08-04\nHija legitima\njuan\nlulia\n...\nvarientos\nNaN\nNaN\nNaN\nNaN\nPampamarca\nNaN\nPampamarca\nNaN\nNaN\n\n\n2\nAPAucará LB L001\nB003\nBautizo\n1790-10-07\nbartola\nNaN\n1790-08-04\nHija legitima\njacinto\nquispe\n...\nNaN\nNaN\nrotonda\npocco\nNaN\nPampamarca\nNaN\nPampamarca\nNaN\nNaN\n\n\n3\nAPAucará LB L001\nB004\nBautizo\n1790-10-20\nfrancisca\nNaN\n1790-10-15\nHija legitima\njuan\ncuebas\n...\nNaN\nNaN\nysabel\nguillen\nNaN\nAucara\nNaN\nNaN\nNaN\nNaN\n\n\n4\nAPAucará LB L001\nB005\nBautizo\n1790-10-20\npedro\nNaN\n1790-10-19\nHijo legitimo\nsantos\nmanxo\n...\nNaN\nNaN\njosefa\nsantiago\nNaN\nAucara\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 26 columns\n\n\n\n\nMATRIMONIOS_HARMONIZED.head()\n\n\n\n\n\n\n\n\nfile\nidentifier\nevent_type\nevent_date\ngroom_name\ngroom_lastname\ngroom_social_condition\ngroom_marital_status\ngroom_age\ngroom_hometown\n...\nwitness_3_lastname\nwitness_4_name\nwitness_4_lastname\nevent_place\nevent_geographic_descriptor_1\nevent_geographic_descriptor_2\nevent_geographic_descriptor_3\nevent_geographic_descriptor_4\nevent_geographic_descriptor_5\nevent_geographic_descriptor_6\n\n\n\n\n0\nAPAucará LM L001\nM001\nMatrimonio\n1816-12-06\njosé manl manuel\nde la roca\ndon, vecinos de este pueblo [Aucara]\nsoltero\nNaN\nCiudad de Huamanga\n...\nbaldes\nNaN\nNaN\nAucara\nNaN\nHuamanga\nNaN\nNaN\nNaN\nNaN\n\n\n1\nAPAucará LM L001\nM002\nMatrimonio\n1816-12-12\nesteban\ncastillo\nvecinos de esta doctrina [Aucara]\nsoltero\nNaN\nNaN\n...\nguamani\nNaN\nNaN\nAucara\nNaN\nColca\nNaN\nNaN\nNaN\nNaN\n\n\n2\nAPAucará LM L001\nM003\nMatrimonio\n1817-03-05\nalexandro\nramires\nvecinos de dicho pueblo [Aucara]\nsoltero\nNaN\nPampamarca\n...\nurbano\nNaN\nNaN\nAucara\nNaN\nPampamarca\nNaN\nNaN\nNaN\nNaN\n\n\n3\nAPAucará LM L001\nM004\nMatrimonio\n1817-03-10\njose\ncuchu\nvecinos de dicho [Pampamarca]\nsoltero\nNaN\nPampamarca\n...\ncoillo\nNaN\nNaN\nPampamarca|santa iglesia\nNaN\nPampamarca\nNaN\nNaN\nNaN\nNaN\n\n\n4\nAPAucará LM L001\nM005\nMatrimonio\n1817-03-12\ndomingo\ntito\nvecinos de dicho [Pampamarca]\nsoltero\nNaN\nNaN\n...\nguallpatuiru\nNaN\nNaN\nPampamarca|santa iglesia\nNaN\nPampamarca\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 55 columns\n\n\n\n\nENTIERROS_HARMONIZED.head()\n\n\n\n\n\n\n\n\nfile\nidentifier\nevent_type\nevent_date\ndoctrine\nevent_place\ndeceased_name\ndeceased_lastname\ndeceased_age\ndeceased_hometown\n...\nfather_lastname\nmother_name\nmother_lastname\nhusband_name\nwife_name\nburial_place\nevent_geographic_descriptor_1\nevent_geographic_descriptor_2\nevent_geographic_descriptor_3\nevent_geographic_descriptor_4\n\n\n\n\n0\nAPAucará LD L001\nE001\nEntierro\n1846-10-06\nParroquia de Aucará\nNaN\njulian\nxavies\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nmercedes lupa\nNaN\nNaN\nLucanas\nNaN\nNaN\n\n\n1\nAPAucará LD L001\nE002\nEntierro\n1846-10-07\nParroquia de Aucará\nNaN\njoce\nraime\n1821-10-13\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nfrancisca cucho\nNaN\nNaN\nLucanas\nNaN\nNaN\n\n\n2\nAPAucará LD L001\nE003\nEntierro\n1846-11-02\nParroquia de Aucará\nNaN\nmartina\ncondori\n1766-11-21\nNaN\n...\nNaN\nNaN\nNaN\nluciano ccoyllo\nNaN\nNaN\nNaN\nLucanas\nNaN\nNaN\n\n\n3\nAPAucará LD L001\nE004\nEntierro\n1846-12-08\nParroquia de Aucará\nNaN\ndorotea\nccoyllo\n1806-12-18\nNaN\n...\nNaN\nNaN\nNaN\njosé espinosa\nNaN\nNaN\nNaN\nLucanas\nNaN\nNaN\n\n\n4\nAPAucará LD L001\nE005\nEntierro\n1847-02-23\nParroquia de Aucará\nNaN\nmaría\nromani\n1797-03-06\nNaN\n...\nNaN\nNaN\nNaN\nmariano huallpatuiro\nNaN\nNaN\nNaN\nLucanas\nNaN\nNaN\n\n\n\n\n5 rows × 24 columns"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "Sondondo Data Project\nThis is a collection of notebooks and documentation for the Sondondo historical data processing project."
  },
  {
    "objectID": "3_termExtraction.html",
    "href": "3_termExtraction.html",
    "title": "Textual Variation Analysis for ‘Conditions’ Columns",
    "section": "",
    "text": "This analysis builds upon ealier iterations using textAnalysis.py and textualAnalysis.py which led to the creation of the current conditionMapping.json.\nThe goal of this analysis is to examine textual variation and overlapping categories within the “conditions” columns across the cleaned datasets (data/clean/bautismos_clean.csv, data/clean/matrimonios_clean.csv, and data/clean/defunciones_clean.csv). This helps to evaluate how a reduced and harmonized mapping contributes to a more consistent and maneageable classification scheme."
  },
  {
    "objectID": "3_termExtraction.html#strategy-for-creating-conditionmapping.json",
    "href": "3_termExtraction.html#strategy-for-creating-conditionmapping.json",
    "title": "Textual Variation Analysis for ‘Conditions’ Columns",
    "section": "Strategy for Creating conditionMapping.json",
    "text": "Strategy for Creating conditionMapping.json\nThe main objective was to reduce textual variation in the “conditions” columns without inadvertently losing important information. The process involved the following steps:\n\nDefine three main categories of “conditions”:\n\nLegitimacy status: Wheter the child was born within a legitimate marriage.\nMarital status: Wheter a persona was married, divorced, or widowed at the time of the event.\nSocial condition: Labels used by the authority recording the event to mark social origin or perceived status (e.g., “indigena”, “mestizo”).\n\nRun TF-IDF analysis across all “conditions” columns from the raw datasets to identify the most frequent and distinctive keywords in each group.\n\nResults can be found in main@e3d7781/logs/textanalysis.log\n\nManually curate mapping from the top-ranked terms for each category. The following criteria guided the normalization process:\n\nRemove diacritics and accents.\nConvert all gendered terms to their masculine form.\nUse singular rather than plural forms.\nExpand abbreviations to full terms.\nConsolidate synonyms into the most commonly used or historically accurate term, e.g., “natural” mapped to “indio”."
  },
  {
    "objectID": "3_termExtraction.html#set-the-project",
    "href": "3_termExtraction.html#set-the-project",
    "title": "Textual Variation Analysis for ‘Conditions’ Columns",
    "section": "Set the Project",
    "text": "Set the Project\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re\nimport json\n\nfrom utils.LoggerHandler import setup_logger\n\nlogger = setup_logger(\"termExtraction\")\n\n\ndataframes_paths = {\n    \"bautismos\": {\n        \"csv_file\": \"../data/clean/bautismos_clean.csv\"\n    },\n    \"entierros\": {\n        \"csv_file\": \"../data/clean/entierros_clean.csv\"\n    },\n    \"matrimonios\": {\n        \"csv_file\": \"../data/clean/matrimonios_clean.csv\"\n    }\n}\n\nwith open('../data/mappings/conditionMapping.json', 'r', encoding='utf-8') as f:\n    condition_mappings = json.load(f)\n\nexisting_mappings = {\n    \"legitimacy_status\": list(condition_mappings[\"attribute_mappings\"][\"legitimacy_status\"].keys()),\n    \"social_condition\": list(condition_mappings[\"attribute_mappings\"][\"social_condition\"].keys()),\n    \"marital_status\": list(condition_mappings[\"attribute_mappings\"][\"marital_status\"].keys())\n}"
  },
  {
    "objectID": "3_termExtraction.html#statistical-analysis",
    "href": "3_termExtraction.html#statistical-analysis",
    "title": "Textual Variation Analysis for ‘Conditions’ Columns",
    "section": "Statistical Analysis",
    "text": "Statistical Analysis\nTo accurately assess the effectiveness of the mapping, we performed a series of statistical analyses on the cleaned datasets.\n\nTextual Variation Extraction\nThese methods extract textual variations from the “conditions” columns using a regex pattern that matches the defined categories.\n\ndef extract_textual_variations_from_columns(column_pattern, dataframes_paths, min_frequency: int = 1) -&gt; dict:\n    \"\"\"\n    Extract all textual variations from columns matching a pattern.\n    This reveals how conditions were expressed in historical documents,\n    including mixed categories (e.g., marital status + age descriptors).\n    \n    Args:\n        column_pattern: Regex pattern to match column names\n        min_frequency: Minimum frequency for a term to be included\n        \n    Returns:\n        Dictionary with term frequencies and contextual metadata\n    \"\"\"\n    term_data = {}\n\n    for dataset, info in dataframes_paths.items():\n        logger.info(f\"Extracting textual variations from {dataset}...\")\n        csv_path = info[\"csv_file\"]\n\n        df = pd.read_csv(csv_path)\n\n        # Find matching columns\n        if isinstance(column_pattern, str):\n            pattern = re.compile(column_pattern)\n        else:\n            pattern = column_pattern\n            \n        matching_columns = [col for col in df.columns if pattern.search(col)]\n        logger.info(f\"Columns found: {matching_columns}\")\n\n        if not matching_columns:\n            continue\n        \n        # Extract all textual variations\n        for col in matching_columns:\n            logger.info(f\"Processing column: {col}\")\n            \n            # basic preprocessing\n            values = df[col].dropna().astype(str)\n            values = values[values != 'nan']\n            values = values[values.str.strip() != '']\n            \n            cleaned_values = values.str.lower().str.strip()\n\n            logger.info(f\"Found {len(cleaned_values)} non-empty entries\")\n\n            # Count occurrences\n            value_counts = cleaned_values.value_counts()\n            \n            for term, count in value_counts.items():\n                if term not in term_data:\n                    term_data[term] = {\n                        'frequency': 0, \n                        'columns': set(), \n                        'datasets': set(),\n                        'raw_variations': set()\n                    }\n                \n                term_data[term]['frequency'] += count\n                term_data[term]['columns'].add(col)\n                term_data[term]['datasets'].add(dataset)\n                \n                original_values = values[cleaned_values == term]\n                term_data[term]['raw_variations'].update(original_values.tolist())\n\n    # Filter by frequency\n    filtered_terms = {}\n    for term, data in term_data.items():\n        if data['frequency'] &gt;= min_frequency:\n            filtered_terms[term] = {\n                'frequency': data['frequency'],\n                'columns': list(data['columns']),\n                'datasets': list(data['datasets']),\n                'raw_variations': list(data['raw_variations'])\n            }\n    \n    return filtered_terms\n\n\n# Categories Data\n\nlegitimacy_variations = extract_textual_variations_from_columns(\n    column_pattern=re.compile(r\".*legitimacy_status.*\"),\n    min_frequency=1,\n    dataframes_paths=dataframes_paths\n)\n\nsocial_variations = extract_textual_variations_from_columns(\n    column_pattern=re.compile(r\".*social_condition.*\"),\n    min_frequency=1,\n    dataframes_paths=dataframes_paths\n)\n\nmarital_variations = extract_textual_variations_from_columns(\n    column_pattern=re.compile(r\".*marital_status.*\"),\n    min_frequency=1,\n    dataframes_paths=dataframes_paths\n)\n\n\ncategories_data = {\n    'legitimacy_status': (legitimacy_variations, existing_mappings[\"legitimacy_status\"]),\n    'social_condition': (social_variations, existing_mappings[\"social_condition\"]),\n    'marital_status': (marital_variations, existing_mappings[\"marital_status\"])\n}\n\n\n\nResults of the Exploratory Analysis\nWe performed a frequency count of the terms found in the “conditions” columns, grouping them by their main categories. The following results summarize the number of unique terms and their total frequency of appearance in the dataset:\n\nstats = {}\n\nfor category, (terms_dict, mappings) in categories_data.items():\n    frequencies = [info['frequency'] for info in terms_dict.values()]\n    stats[category] = {\n        'frequencies': frequencies,\n        'total_terms': len(frequencies),\n        'total_frequency': sum(frequencies)\n    }\n\nstats_df = pd.DataFrame(stats).T.sort_values(by='total_frequency', ascending=False).reset_index().rename(columns={'index': 'category'})\nstats_df\n\n\n\n\n\n\n\n\ncategory\nfrequencies\ntotal_terms\ntotal_frequency\n\n\n\n\n0\nlegitimacy_status\n[996, 917, 1495, 1366, 521, 428, 383, 350, 118...\n314\n10265\n\n\n1\nsocial_condition\n[1382, 332, 29, 14, 18, 8, 33, 10, 71, 6, 6, 6...\n955\n9249\n\n\n2\nmarital_status\n[400, 247, 406, 1389, 1321, 368, 24, 24, 18, 1...\n34\n4254\n\n\n\n\n\n\n\nThe data shows that the social condition category has the highest number of unique terms (955), despite having a lower total frequency than legitimacy status. This suggests that many of the terms under social condition appear only once or a few times, indicating high textual variation and inconsistency — likely due to diverse wording choices by different scribes over time, as well as inconsistencies introduced during transcription.\nIn contrast, the marital status category is significantly more standardized, with only 34 unique terms used across 4,254 entries. This low variation points to a more consistent vocabulary and higher likelihood of successful harmonization.\nTo further quantify this, we computed the average frequency per term, which helps identify how often terms are reused — a proxy for redundancy and consistency.\n\nstats_df['avg_frequency_per_term'] = stats_df['total_frequency'] / stats_df['total_terms']\nstats_df['avg_frequency_per_term'] = stats_df['avg_frequency_per_term'].astype(float).round(2)\nstats_df\n\n\n\n\n\n\n\n\ncategory\nfrequencies\ntotal_terms\ntotal_frequency\navg_frequency_per_term\n\n\n\n\n0\nlegitimacy_status\n[996, 917, 1495, 1366, 521, 428, 383, 350, 118...\n314\n10265\n32.69\n\n\n1\nsocial_condition\n[1382, 332, 29, 14, 18, 8, 33, 10, 71, 6, 6, 6...\n955\n9249\n9.68\n\n\n2\nmarital_status\n[400, 247, 406, 1389, 1321, 368, 24, 24, 18, 1...\n34\n4254\n125.12\n\n\n\n\n\n\n\nAs shown above:\n\nSocial condition has the lowest average frequency per term (~9.7), confirming high fragmentation and the presence of long-tail entries that may be semantically similar but textually distinct.\nMarital status has the highest reuse rate (~125 per term), suggesting a more constrained vocabulary and consistent usage patterns.\nLegitimacy status sits in between, with moderate variation and moderate reuse.\n\nWhile all three categories benefit from textual normalization, social condition stands out as requiring special attention. Its high number of unique, low-frequency terms suggests significant noise and ambiguity that can hinder downstream analysis unless harmonized. The average frequency metric supports this interpretation and can serve as a quantifiable justification for the harmonization strategy applied in subsequent steps.\n\n\nCoefficient of Variation:\nThe coefficient of variation (CV) measures how much variability exists relative to the average value. In our case, we compute the CV across parishes based on the number of unique “conditions” entries before and after applying the mapping. A lower CV after mapping indicates that the mapping has reduced variation in the use of terms across parishes, suggesting improved consistency.\n\nfrom utils import StatsMethods\n\n\nstats_df['cv'] = stats_df.apply(lambda row: StatsMethods.cv(row['frequencies'], rounding=2), axis=1)\nstats_df\n\n\n\n\n\n\n\n\ncategory\nfrequencies\ntotal_terms\ntotal_frequency\navg_frequency_per_term\ncv\n\n\n\n\n0\nlegitimacy_status\n[996, 917, 1495, 1366, 521, 428, 383, 350, 118...\n314\n10265\n32.69\n4.96\n\n\n1\nsocial_condition\n[1382, 332, 29, 14, 18, 8, 33, 10, 71, 6, 6, 6...\n955\n9249\n9.68\n5.66\n\n\n2\nmarital_status\n[400, 247, 406, 1389, 1321, 368, 24, 24, 18, 1...\n34\n4254\n125.12\n2.66\n\n\n\n\n\n\n\n\n\nShannon Entropy\nShannon entropy quantifies the uncertainty or randomness in a set of data. We can notice that social condition has the highest entropy (H) and normalized entropy (Normalized_H), showing again the level of fragmentation of this group. Legitimacy and marital status have lower levels, but we expect that the mapping will reduce their entropy as well, leading to more consistent usage patterns.\n\nstats_df[['H', 'H_max', 'Normalized_H', 'Redundancy']] = stats_df.apply(\n    lambda row: StatsMethods.shannon_entropy(row['frequencies'], rounding=2),\n    axis=1, result_type='expand'\n)\nstats_df[['cv', 'H', 'H_max', 'Normalized_H', 'Redundancy']]\n\n\n\n\n\n\n\n\ncv\nH\nH_max\nNormalized_H\nRedundancy\n\n\n\n\n0\n4.96\n4.43\n8.29\n0.53\n0.47\n\n\n1\n5.66\n6.97\n9.90\n0.70\n0.30\n\n\n2\n2.66\n2.50\n5.09\n0.49\n0.51\n\n\n\n\n\n\n\n\n\nZipf Distribution\nZipf’s law describes the frequency of terms in a language, where a few terms are very common while many others are rare. We can visualize the distribution of term frequencies to see if they follow this pattern. A Zipfian distribution would indicate that a small number of terms dominate usage, while many others are used infrequently.\n\nstats_df['zipf_distribution'] = stats_df.apply(\n    lambda row: StatsMethods.zipf_distribution(row['frequencies'], rounding=2),\n    axis=1\n)\n\nstats_df['empirical_ranks'] = stats_df.apply(\n    lambda row: StatsMethods.empirical_rank_freq(row['frequencies'], normalize=True, rounding=2),\n    axis=1\n)\n\nstats_df[['cv', 'H', 'H_max', 'Normalized_H', 'Redundancy', 'zipf_distribution', 'empirical_ranks']]\n\n\n\n\n\n\n\n\ncv\nH\nH_max\nNormalized_H\nRedundancy\nzipf_distribution\nempirical_ranks\n\n\n\n\n0\n4.96\n4.43\n8.29\n0.53\n0.47\n[0.16, 0.08, 0.05, 0.04, 0.03, 0.03, 0.02, 0.0...\n[0.15, 0.13, 0.1, 0.09, 0.09, 0.09, 0.05, 0.04...\n\n\n1\n5.66\n6.97\n9.90\n0.70\n0.30\n[0.13, 0.07, 0.04, 0.03, 0.03, 0.02, 0.02, 0.0...\n[0.15, 0.06, 0.05, 0.04, 0.03, 0.02, 0.02, 0.0...\n\n\n2\n2.66\n2.50\n5.09\n0.49\n0.51\n[0.24, 0.12, 0.08, 0.06, 0.05, 0.04, 0.03, 0.0...\n[0.33, 0.31, 0.1, 0.09, 0.09, 0.06, 0.01, 0.01...\n\n\n\n\n\n\n\n\ndef get_series(df, cat, col):\n    return df.loc[df['category'] == cat, col].iloc[0]\n\ncats   = ['social_condition','legitimacy_status','marital_status']\ntitles = ['Social Condition','Legitimacy Status','Marital Status']\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 4), sharey=False)\n\nfor ax, cat, title in zip(axes, cats, titles):\n    z_before = get_series(stats_df, cat, 'zipf_distribution')\n    e_before = get_series(stats_df, cat, 'empirical_ranks')\n\n\n    # x = ranks (start at 1)\n    xb = np.arange(1, len(e_before) + 1)\n\n    # empirical \n    ax.loglog(xb, e_before, marker='o', linestyle='-',  label='Empirical')\n\n    # theoretical Zipf  (dashed)\n    ax.loglog(xb, z_before, linestyle='--', label='Zipf ref')\n\n    ax.set_title(title)\n    ax.set_xlabel(\"Rank (descending frequency)\")\n    ax.grid(True, which='both', linestyle=':', alpha=0.5)\n\naxes[0].set_ylabel(\"Probability (normalized frequency)\")\naxes[0].legend() \nplt.suptitle(\"Empirical Rank-Frequency vs Theoretical Zipf\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "3_termExtraction.html#normalization",
    "href": "3_termExtraction.html#normalization",
    "title": "Textual Variation Analysis for ‘Conditions’ Columns",
    "section": "Normalization",
    "text": "Normalization\nOur normalization strategy is designed to address the textual variation, redundancy, high entropy, and category mixing present in the “conditions” columns. The label “condition” was ambiguous even for parish priests and local officials, leading to inconsistent application across records. Without a standardized vocabulary or a consistent method for disambiguating terms, social, marital, and legitimacy-related descriptors appear intermingled in the same fields.\nTo resolve this, we developed the InferCondition.py module, which can extract specific attributes from any “conditions” column, regardless of the category in which they were recorded. For example, extract_marital_status can be applied to a social_condition column to recover marital status terms that were incorrectly placed there. When applied to the correct category, these extractors significantly reduce variability.\nAt its core, the module implements the AttributeNormalizer class, which:\n\nLoads predefined mapping dictionaries for social condition, legitimacy status, and marital status.\nApplies a multi-stage harmonization process:\n\nCase-insensitive exact match (checks if the term already matches a target form).\nWord-level match (matches any single token to the mapping).\nSubstring match (captures longer phrases containing mapped terms).\nFuzzy match (using RapidFuzz with a configurable threshold).\n\nLogs unmapped values for later review.\nProvides convenience methods to extract all attributes from a column or harmonize multiple columns in one pass.\nIncludes a function to extract unmapped tokens, removing all known mapped values and leaving residual, potentially meaningful text for further analysis.\n\nThis normalization is a foundational step in preparing the dataset for probabilistic record linkage, as it reduces noise, improves consistency, and aligns variant expressions to a controlled vocabulary without discarding historically meaningful distinctions.\n\nThis evaluation measures the effectiveness of the mapping only when applied to the correct category. It does not yet account for misfiled attributes across categories at the individual record level. A more comprehensive parsing and normalization process — applied directly to individual persona records — will be implemented in subsequent steps.\n\n\nfrom actions.generators import InferCondition\n\n\ndef normalize_textual_variations_from_columns(dataframes_paths, mapping_file, threshold=80):\n    \"\"\"\n    Normalize textual variations in multiple columns based on a mapping file.\n\n    Args:\n        column_pattern: Regex pattern to match column names\n        mapping_file: Path to the JSON mapping file\n        threshold: Level of fuzziness for matching terms (higher values are more strict)\n    \"\"\"\n\n    inferer = InferCondition.AttributeNormalizer(mapping_file=mapping_file, fuzzy_threshold=threshold)\n\n    for dataset, info in dataframes_paths.items():\n        logger.info(f\"Normalizing ALL categories in {dataset} (single pass)...\")\n        df = pd.read_csv(info[\"csv_file\"])\n\n        social_cols = [c for c in df.columns if c.lower().endswith(\"_social_condition\") or c.lower().endswith(\"_condition\")]\n        for col in social_cols:\n            logger.info(f\"Normalizing SOCIAL column: {col}\")\n            df[col] = inferer.extract_social_condition(df[col])\n\n        legitimacy_cols = [c for c in df.columns if c.lower().endswith(\"_legitimacy_status\")]\n        for col in legitimacy_cols:\n            logger.info(f\"Normalizing LEGITIMACY column: {col}\")\n            df[col] = inferer.extract_legitimacy_status(df[col])\n\n        marital_cols = [c for c in df.columns if c.lower().endswith(\"_marital_status\")]\n        for col in marital_cols:\n            logger.info(f\"Normalizing MARITAL column: {col}\")\n            df[col] = inferer.extract_marital_status(df[col])\n\n        df.to_csv(f\"../data/interim/{dataset}_normalized.csv\", index=False)\n    logger.info(\"Normalization complete.\")\n\n\nnormalize_textual_variations_from_columns(\n    dataframes_paths=dataframes_paths,\n    mapping_file=condition_mappings,\n    threshold=80,\n)\n\n\n\nnormalize_dataframes_paths = {\n    \"bautismos\": {\n        \"csv_file\": \"../data/interim/bautismos_normalized.csv\"\n    },\n    \"entierros\": {\n        \"csv_file\": \"../data/interim/entierros_normalized.csv\"\n    },\n    \"matrimonios\": {\n        \"csv_file\": \"../data/interim/matrimonios_normalized.csv\"\n    }\n}\n\n\nlegitimacy_variations_normalized = extract_textual_variations_from_columns(\n    column_pattern=re.compile(r\".*legitimacy_status.*\"),\n    min_frequency=1,\n    dataframes_paths=normalize_dataframes_paths\n)\n\nsocial_variations_normalized = extract_textual_variations_from_columns(\n    column_pattern=re.compile(r\".*social_condition.*\"),\n    min_frequency=1,\n    dataframes_paths=normalize_dataframes_paths\n)\n\nmarital_variations_normalized = extract_textual_variations_from_columns(\n    column_pattern=re.compile(r\".*marital_status.*\"),\n    min_frequency=1,\n    dataframes_paths=normalize_dataframes_paths\n)\n\n\ncategories_data_normalized = {\n    'legitimacy_status': (legitimacy_variations_normalized, existing_mappings[\"legitimacy_status\"]),\n    'social_condition': (social_variations_normalized, existing_mappings[\"social_condition\"]),\n    'marital_status': (marital_variations_normalized, existing_mappings[\"marital_status\"])\n}\n\n\nstats_normalized = {}\nfor category, (terms_dict, mappings) in categories_data_normalized.items():\n    frequencies = [info['frequency'] for info in terms_dict.values()]\n    stats_normalized[category] = {\n        'frequencies': frequencies,\n        'total_terms': len(frequencies),\n        'total_frequency': sum(frequencies)\n    }\nstats_normalized_df = pd.DataFrame(stats_normalized).T.sort_values(by='total_frequency', ascending=False).reset_index().rename(columns={'index': 'category'})\nstats_normalized_df\n\n\n\n\n\n\n\n\ncategory\nfrequencies\ntotal_terms\ntotal_frequency\n\n\n\n\n0\nlegitimacy_status\n[8925, 1278]\n2\n10203\n\n\n1\nsocial_condition\n[1663, 901, 43, 107, 4435, 255, 655]\n7\n8059\n\n\n2\nmarital_status\n[692, 777, 2715]\n3\n4184\n\n\n\n\n\n\n\n\nHow much information is lost?\nAt first glance, the results show that the implementation successfully reduced dimensionality with minimal loss of information. Social condition—the most fragmented category—was reduced to just 7 unique terms, with only a 12.87% decrease in total frequency. Considering that the algorithm is designed to return None when no match is found, this reduction is both expected and encouraging, indicating that the normalization process retained most of the original data while greatly improving consistency.\n\nclean_data = stats_df['total_frequency'].astype(float)\nclean_data_normalized = stats_normalized_df['total_frequency'].astype(float)\n\n# percentage of information lost\npercentage_lost = pd.DataFrame({\n    'category': stats_df['category'],\n    'percentage_lost': ((clean_data - clean_data_normalized) / clean_data * 100).round(2)\n})\npercentage_lost\n\n\n\n\n\n\n\n\ncategory\npercentage_lost\n\n\n\n\n0\nlegitimacy_status\n0.60\n\n\n1\nsocial_condition\n12.87\n\n\n2\nmarital_status\n1.65"
  },
  {
    "objectID": "3_termExtraction.html#coefficient-of-variation-after-normalization",
    "href": "3_termExtraction.html#coefficient-of-variation-after-normalization",
    "title": "Textual Variation Analysis for ‘Conditions’ Columns",
    "section": "Coefficient of Variation After Normalization",
    "text": "Coefficient of Variation After Normalization\nThe coefficient of variation (CV) analysis confirms a substantial reduction in dispersion across categories after normalization. For legitimacy status, CV dropped from 4.96 to 1.06; for social condition, from 5.66 to 1.35; and for marital status, from 2.66 to 0.82. This indicates that the normalized categories are considerably more homogeneous, reducing variability while preserving most of the original data volume.\n\nstats_normalized_df['cv'] = stats_normalized_df.apply(lambda row: StatsMethods.cv(row['frequencies'], rounding=2), axis=1)\n\nstats_normalized_df\n\n\n\n\n\n\n\n\ncategory\nfrequencies\ntotal_terms\ntotal_frequency\ncv\n\n\n\n\n0\nlegitimacy_status\n[8925, 1278]\n2\n10203\n1.06\n\n\n1\nsocial_condition\n[1663, 901, 43, 107, 4435, 255, 655]\n7\n8059\n1.35\n\n\n2\nmarital_status\n[692, 777, 2715]\n3\n4184\n0.82\n\n\n\n\n\n\n\n\ncv_comparison = pd.DataFrame({\n    'category': stats_df['category'],\n    'cv_before': stats_df['cv'],\n    'cv_after': stats_normalized_df['cv']\n})\ncv_comparison\n\n\n\n\n\n\n\n\ncategory\ncv_before\ncv_after\n\n\n\n\n0\nlegitimacy_status\n4.96\n1.06\n\n\n1\nsocial_condition\n5.66\n1.35\n\n\n2\nmarital_status\n2.66\n0.82"
  },
  {
    "objectID": "3_termExtraction.html#shannon-entropy-after-normalization",
    "href": "3_termExtraction.html#shannon-entropy-after-normalization",
    "title": "Textual Variation Analysis for ‘Conditions’ Columns",
    "section": "Shannon Entropy After Normalization",
    "text": "Shannon Entropy After Normalization\nThe coefficient of variation (CV) indicates that the data is more consistent after normalization, while Shannon entropy shows a reduction in unpredictability. Notably, the normalized entropy for marital status increased from 0.49 to 0.81, suggesting that the relative balance among categories has improved substantially. Overall, the normalized entropy results demonstrate that the mapping not only improved the balance of categories but also reduced the clutter and fragmentation in the “conditions” columns.\n\nstats_normalized_df[['H', 'H_max', 'Normalized_H', 'Redundancy']] = stats_normalized_df.apply(\n    lambda row: StatsMethods.shannon_entropy(row['frequencies'], rounding=2),\n    axis=1, result_type='expand'\n)\nstats_normalized_df[['cv', 'H', 'H_max', 'Normalized_H', 'Redundancy']]\n\n\n\n\n\n\n\n\ncv\nH\nH_max\nNormalized_H\nRedundancy\n\n\n\n\n0\n1.06\n0.54\n1.00\n0.54\n0.46\n\n\n1\n1.35\n1.87\n2.81\n0.67\n0.33\n\n\n2\n0.82\n1.29\n1.58\n0.81\n0.19\n\n\n\n\n\n\n\n\nentropy_comparison = pd.DataFrame({\n    'category': stats_df['category'],\n    'H_before': stats_df['H'],\n    'H_after': stats_normalized_df['H'],\n    'H_max_before': stats_df['H_max'],\n    'H_max_after': stats_normalized_df['H_max'],\n    'Normalized_H_before': stats_df['Normalized_H'],\n    'Normalized_H_after': stats_normalized_df['Normalized_H'],\n    'Redundancy_before': stats_df['Redundancy'],\n    'Redundancy_after': stats_normalized_df['Redundancy']\n})\nentropy_comparison\n\n\n\n\n\n\n\n\ncategory\nH_before\nH_after\nH_max_before\nH_max_after\nNormalized_H_before\nNormalized_H_after\nRedundancy_before\nRedundancy_after\n\n\n\n\n0\nlegitimacy_status\n4.43\n0.54\n8.29\n1.00\n0.53\n0.54\n0.47\n0.46\n\n\n1\nsocial_condition\n6.97\n1.87\n9.90\n2.81\n0.70\n0.67\n0.30\n0.33\n\n\n2\nmarital_status\n2.50\n1.29\n5.09\n1.58\n0.49\n0.81\n0.51\n0.19\n\n\n\n\n\n\n\n\ncategories = stats_df['category'].tolist()\n\ncv_before = stats_df['cv']\ncv_after = stats_normalized_df['cv']\n\nnh_before = stats_df['Normalized_H']\nnh_after = stats_normalized_df['Normalized_H']\n\nx = np.arange(len(categories))\nwidth = 0.35 \n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n# Plot CV\naxes[0].bar(x - width/2, cv_before, width, label='Before', color='skyblue')\naxes[0].bar(x + width/2, cv_after, width, label='After', color='orange')\naxes[0].set_title(\"Coefficient of Variation (CV)\")\naxes[0].set_xticks(x)\naxes[0].set_xticklabels(categories, rotation=20)\naxes[0].set_ylabel(\"CV\")\naxes[0].legend()\naxes[0].grid(axis='y', linestyle='--', alpha=0.6)\n\n# Plot Normalized Entropy\naxes[1].bar(x - width/2, nh_before, width, label='Before', color='skyblue')\naxes[1].bar(x + width/2, nh_after, width, label='After', color='orange')\naxes[1].set_title(\"Normalized Entropy (H/Hmax)\")\naxes[1].set_xticks(x)\naxes[1].set_xticklabels(categories, rotation=20)\naxes[1].set_ylabel(\"Normalized Entropy\")\naxes[1].legend()\naxes[1].grid(axis='y', linestyle='--', alpha=0.6)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nZipf Distribution After Normalization\nAfter this radical reduction of variability, the empirical rank–frequency curves move closer to the expected Zipfian distribution, indicating that the normalization reduced fragmentation and produced a more coherent distribution of terms.\n\nLegitimacy status: The deviation from Zipf is due to the binary nature of the attribute (e.g., legitimate vs. illegitimate), not to residual noise. With only two valid terms, the imbalance between them is expected and does not affect matchability.\nSocial condition: The original long tail of rare, inconsistent terms has been greatly compressed into a small set of high-frequency categories, bringing the empirical curve into closer alignment with the theoretical model.\nMarital status: The observed mid-rank deviation from the theoretical Zipf curve reflects the small size of the category set (three terms) rather than residual noise. In such small vocabularies, differences in the natural prevalence of each term (e.g., soltero more common than viudo) produce rank–frequency shapes that diverge from the ideal Zipf slope.\n\n\nstats_normalized_df['zipf_distribution'] = stats_normalized_df.apply(\n    lambda row: StatsMethods.zipf_distribution(row['frequencies'], rounding=2),\n    axis=1\n)\n\nstats_normalized_df['empirical_ranks'] = stats_normalized_df.apply(\n    lambda row: StatsMethods.empirical_rank_freq(row['frequencies'], normalize=True, rounding=2),\n    axis=1\n)\n\nstats_normalized_df[['cv', 'H', 'H_max', 'Normalized_H', 'Redundancy', 'zipf_distribution', 'empirical_ranks']]\n\n\n\n\n\n\n\n\ncv\nH\nH_max\nNormalized_H\nRedundancy\nzipf_distribution\nempirical_ranks\n\n\n\n\n0\n1.06\n0.54\n1.00\n0.54\n0.46\n[0.67, 0.33]\n[0.87, 0.13]\n\n\n1\n1.35\n1.87\n2.81\n0.67\n0.33\n[0.39, 0.19, 0.13, 0.1, 0.08, 0.06, 0.06]\n[0.55, 0.21, 0.11, 0.08, 0.03, 0.01, 0.01]\n\n\n2\n0.82\n1.29\n1.58\n0.81\n0.19\n[0.55, 0.27, 0.18]\n[0.65, 0.19, 0.17]\n\n\n\n\n\n\n\n\ndef get_series(df, cat, col):\n    return df.loc[df['category'] == cat, col].iloc[0]\n\ncategories = stats_df['category'].tolist()\ntitles = [c.replace('_', ' ').title() for c in categories]\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 4), sharey=False)\n\nfor ax, cat, title in zip(axes, categories, titles):\n    # BEFORE\n    z_before = get_series(stats_df, cat, 'zipf_distribution')\n    e_before = get_series(stats_df, cat, 'empirical_ranks')\n\n    # AFTER (normalized)\n    z_after  = get_series(stats_normalized_df, cat, 'zipf_distribution')\n    e_after  = get_series(stats_normalized_df, cat, 'empirical_ranks')\n\n    # x = ranks (start at 1)\n    xb = np.arange(1, len(e_before) + 1)\n    xa = np.arange(1, len(e_after)  + 1)\n\n    # empirical before/after\n    ax.loglog(xb, e_before, marker='o', linestyle='-',  label='Before (empirical)')\n    ax.loglog(xa, e_after,  marker='o', linestyle='-',  label='After (empirical)')\n\n    # theoretical Zipf before/after (dashed)\n    ax.loglog(xb, z_before, linestyle='--', label='Zipf ref (before)')\n    ax.loglog(xa, z_after,  linestyle='--', label='Zipf ref (after)')\n\n    ax.set_title(title)\n    ax.set_xlabel(\"Rank (descending frequency)\")\n    ax.grid(True, which='both', linestyle=':', alpha=0.5)\n\naxes[0].set_ylabel(\"Probability (normalized frequency)\")\naxes[0].legend()  # legend on first panel to save space\nplt.suptitle(\"Empirical Rank–Frequency vs Theoretical Zipf (Before vs After)\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nfig, axes = plt.subplots(1, len(categories), figsize=(15, 4), sharey=True)\n\nfor i, category in enumerate(categories):\n    theoretical = stats_normalized_df.loc[stats_normalized_df['category'] == category, 'zipf_distribution'].iloc[0]\n    empirical = stats_normalized_df.loc[stats_normalized_df['category'] == category, 'empirical_ranks'].iloc[0]\n\n    deviation = np.array(empirical) - np.array(theoretical)\n    \n    axes[i].bar(range(1, len(theoretical) + 1), deviation, color='gray', alpha=0.7)\n    axes[i].axhline(0, color='red', linestyle='--', linewidth=1)\n    axes[i].set_title(category)\n    axes[i].set_xlabel(\"Rank\")\n    if i == 0:\n        axes[i].set_ylabel(\"Deviation (Empirical - Zipf)\")\n    \nplt.suptitle(\"Deviation from Theoretical Zipf by Category\", y=1.05)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "3_termExtraction.html#conclusion",
    "href": "3_termExtraction.html#conclusion",
    "title": "Textual Variation Analysis for ‘Conditions’ Columns",
    "section": "Conclusion",
    "text": "Conclusion\nThe normalization process successfully compressed a large, noisy set of textual variations into a reduced, controlled vocabulary. Across all categories, the coefficient of variation decreased and normalized entropy increased, indicating greater term consistency and balance. The empirical rank–frequency curves moved closer to the theoretical Zipf distribution, showing reduced fragmentation and improved coherence in term usage.\nInformation loss was minimal — under 2% for legitimacy status and marital status, and about 13% for social condition, the most fragmented category prior to normalization. These changes substantially reduce noise and improve the dataset’s suitability for probabilistic record linkage.\nObserved deviations from a perfect Zipfian distribution in legitimacy status and marital status are explained by their inherently small vocabularies (binary and three-term sets, respectively), rather than residual inconsistency. Future iterations could selectively expand controlled vocabularies if adding more dimensions proves beneficial for specific linkage scenarios."
  },
  {
    "objectID": "3_termExtraction.html#resulting-data",
    "href": "3_termExtraction.html#resulting-data",
    "title": "Textual Variation Analysis for ‘Conditions’ Columns",
    "section": "Resulting Data",
    "text": "Resulting Data\nThe data produced during this analysis is stored in the data/interim directory for documentation and reproducibility purposes.\n\nstats_df.to_csv(\"../data/interim/term_extraction_stats.csv\", index=False)\nstats_normalized_df.to_csv(\"../data/interim/term_extraction_stats_normalized.csv\", index=False)"
  },
  {
    "objectID": "4_personasCreation.html",
    "href": "4_personasCreation.html",
    "title": "Personas Creation",
    "section": "",
    "text": "Our approach aligns closely with the input preparation model proposed by David W. Embley (2021). We structure our data around personas, defined as “each mention instance of a person in a document” (p. 66), as a foundational step toward probabilistic record linkage (PRL). Each persona is created by ingesting available individual metadata (such as name, last name, birth date), associating the person with a sacramental event (baptism, marriage, or burial), and establishing direct relationships (to parents, spouse, godparents).\nimport pandas as pd\nfrom pathlib import Path"
  },
  {
    "objectID": "4_personasCreation.html#identifying-prefixes",
    "href": "4_personasCreation.html#identifying-prefixes",
    "title": "Personas Creation",
    "section": "Identifying Prefixes",
    "text": "Identifying Prefixes\nThis step is a very simple exercise to ensure consistency and good naming conventions across the datasets. Also helps to identify the entities for the ER model, and associate prefixes to those entities.\n\nimport re\n\nprefix_pattern = re.compile(r\"(^[A-Za-z]*_[\\d]?_?)([A-Za-z]*_?[\\w\\d]*)\")\n\nprefixes = set()\n\nfor df in [bautismos, entierros, matrimonios]:\n    for col in df.columns:\n        if prefix_pattern.match(col):\n            prefix = prefix_pattern.match(col).group(1)\n            remove_pattern = re.compile(r\"\\d\")\n            prefix = remove_pattern.sub(\"\", prefix).strip(\"_\")\n            prefixes.add(prefix)\n\nprefixes\n\n{'baptized',\n 'bride',\n 'burial',\n 'deceased',\n 'event',\n 'father',\n 'godfather',\n 'godmother',\n 'godparent',\n 'groom',\n 'husband',\n 'mother',\n 'parents',\n 'wife',\n 'witness'}"
  },
  {
    "objectID": "4_personasCreation.html#er-model",
    "href": "4_personasCreation.html#er-model",
    "title": "Personas Creation",
    "section": "ER Model",
    "text": "ER Model\nWith those prefixes in mind, and after basic cleaning of data, we can create a more accurate version of the ER model (See original version). The model has now the following structure:\n\n\n\nER Model"
  },
  {
    "objectID": "2_placeMapping.html",
    "href": "2_placeMapping.html",
    "title": "Step 2: Create Unique Values from Data",
    "section": "",
    "text": "# Main Libraries\nimport pandas as pd\n# global variables\nPLACES_MAP = '../data/mappings/places_types.json'\n# Clean Data\n\nBAUTISMOS_HARMONIZED = pd.read_csv(\"../data/clean/bautismos_clean.csv\")\nMATRIMONIOS_HARMONIZED = pd.read_csv(\"../data/clean/matrimonios_clean.csv\")\nENTIERROS_HARMONIZED = pd.read_csv(\"../data/clean/entierros_clean.csv\")\n\nBAUTISMOS_HARMONIZED\n\n\n\n\n\n\n\n\nfile\nidentifier\nevent_type\nevent_date\nbaptized_name\nbaptized_hometown\nbaptized_birth_date\nbaptized_legitimacy_status\nfather_name\nfather_lastname\n...\ngodfather_lastname\ngodfather_social_condition\ngodmother_name\ngodmother_lastname\ngodmother_social_condition\nevent_place\nevent_geographic_descriptor_1\nevent_geographic_descriptor_2\nevent_geographic_descriptor_3\nevent_geographic_descriptor_4\n\n\n\n\n0\nAPAucará LB L001\nB001\nBautizo\n1790-10-04\ndomingo\nNaN\n1790-08-04\nHijo legitimo\nlucas\nayquipa\n...\nguamani\nNaN\nNaN\nNaN\nNaN\nPampamarca\nNaN\nPampamarca\nNaN\nNaN\n\n\n1\nAPAucará LB L001\nB002\nBautizo\n1790-10-06\ndominga\nNaN\n1790-08-04\nHija legitima\njuan\nlulia\n...\nvarientos\nNaN\nNaN\nNaN\nNaN\nPampamarca\nNaN\nPampamarca\nNaN\nNaN\n\n\n2\nAPAucará LB L001\nB003\nBautizo\n1790-10-07\nbartola\nNaN\n1790-08-04\nHija legitima\njacinto\nquispe\n...\nNaN\nNaN\nrotonda\npocco\nNaN\nPampamarca\nNaN\nPampamarca\nNaN\nNaN\n\n\n3\nAPAucará LB L001\nB004\nBautizo\n1790-10-20\nfrancisca\nNaN\n1790-10-15\nHija legitima\njuan\ncuebas\n...\nNaN\nNaN\nysabel\nguillen\nNaN\nAucara\nNaN\nNaN\nNaN\nNaN\n\n\n4\nAPAucará LB L001\nB005\nBautizo\n1790-10-20\npedro\nNaN\n1790-10-19\nHijo legitimo\nsantos\nmanxo\n...\nNaN\nNaN\njosefa\nsantiago\nNaN\nAucara\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n6336\nAPAucará LB L004\nB2042\nBautizo\n1888-12-10\nleocadio\nNaN\n1888-12-09\nHijo natural, mestizo\nmiguel\npacheco\n...\nbendezú\nNaN\nNaN\nNaN\nNaN\nAucará\nNaN\nNaN\nNaN\nNaN\n\n\n6337\nAPAucará LB L004\nB2043\nBautizo\n1888-12-11\nmariano concepcion\nNaN\n1888-12-07\nHijo legítimo, indio\nfacundo\nvega\n...\nmancco\nNaN\nNaN\nNaN\nNaN\nAucará\nNaN\nNaN\nNaN\nNaN\n\n\n6338\nAPAucará LB L004\nB2044\nBautizo\n1888-12-12\nambrosio\nNaN\n1888-12-06\nHijo legítimo, indio\nysidro\nccasane\n...\ntito\nNaN\nNaN\nNaN\nNaN\nAucará\nNaN\nMayobamba\nNaN\nNaN\n\n\n6339\nAPAucará LB L004\nB2045\nBautizo\n1888-12-15\nfrancisco\nNaN\n1888-11-30\nHijo legítimo, indio\nmariano\nlopez\n...\ndias\nIndigna de Huaicahuacho\nNaN\nNaN\nNaN\nAucará\nNaN\nHuaicahuacho\nNaN\nNaN\n\n\n6340\nAPAucará LB L004\nB2046\nBautizo\n1888-12-16\nlaureana\nNaN\n1888-12-01\nHija legítima, india\nbernarda\nchampa\n...\nNaN\nNaN\nmanuela\nde la cruz\nNaN\nAucará\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n6341 rows × 26 columns"
  },
  {
    "objectID": "2_placeMapping.html#map-places",
    "href": "2_placeMapping.html#map-places",
    "title": "Step 2: Create Unique Values from Data",
    "section": "Map Places",
    "text": "Map Places\nCreate a list of unique places from the datasets and create a json file with the mapping of places to their respective geographic descriptors.\n\nfrom actions.extractors import placeRecognition\n\nextractor = placeRecognition.PlaceExtractor()\n\n\nbautismos_place_columns = [\n    'baptized_hometown', 'event_place', 'event_geographic_descriptor_1',\n        'event_geographic_descriptor_2', 'event_geographic_descriptor_3',\n        'event_geographic_descriptor_4'\n]\n\nfor col in bautismos_place_columns:\n    if col in BAUTISMOS_HARMONIZED.columns:\n        BAUTISMOS_HARMONIZED[col] = extractor.extract_places_per_row(BAUTISMOS_HARMONIZED[col])\n\nBAUTISMOS_HARMONIZED[bautismos_place_columns]\n\n\n\n\n\n\n\n\nbaptized_hometown\nevent_place\nevent_geographic_descriptor_1\nevent_geographic_descriptor_2\nevent_geographic_descriptor_3\nevent_geographic_descriptor_4\n\n\n\n\n0\nNaN\nPampamarca\nNaN\nPampamarca\nNaN\nNaN\n\n\n1\nNaN\nPampamarca\nNaN\nPampamarca\nNaN\nNaN\n\n\n2\nNaN\nPampamarca\nNaN\nPampamarca\nNaN\nNaN\n\n\n3\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n6336\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n6337\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n6338\nNaN\nNaN\nNaN\nMayobamba\nNaN\nNaN\n\n\n6339\nNaN\nNaN\nNaN\nHuaicahuacho\nNaN\nNaN\n\n\n6340\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n6341 rows × 6 columns\n\n\n\n\nmatrimonios_place_columns = [\n    'groom_hometown',\n       'groom_resident_in', \n       'bride_hometown', 'bride_resident_in', \n       'event_place', 'event_geographic_descriptor_1', 'event_geographic_descriptor_2',\n       'event_geographic_descriptor_3', 'event_geographic_descriptor_4',\n       'event_geographic_descriptor_5', 'event_geographic_descriptor_6'\n]\n\nfor col in matrimonios_place_columns:\n    if col in MATRIMONIOS_HARMONIZED.columns:\n        MATRIMONIOS_HARMONIZED[col] = extractor.extract_places_per_row(MATRIMONIOS_HARMONIZED[col])\n\nMATRIMONIOS_HARMONIZED[matrimonios_place_columns]\n\n\n\n\n\n\n\n\ngroom_hometown\ngroom_resident_in\nbride_hometown\nbride_resident_in\nevent_place\nevent_geographic_descriptor_1\nevent_geographic_descriptor_2\nevent_geographic_descriptor_3\nevent_geographic_descriptor_4\nevent_geographic_descriptor_5\nevent_geographic_descriptor_6\n\n\n\n\n0\nCiudad de Huamanga\nNaN\nNaN\nNaN\nNaN\nNaN\nHuamanga\nNaN\nNaN\nNaN\nNaN\n\n\n1\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nColca\nNaN\nNaN\nNaN\nNaN\n\n\n2\nPampamarca\nNaN\nPampamarca\nNaN\nNaN\nNaN\nPampamarca\nNaN\nNaN\nNaN\nNaN\n\n\n3\nPampamarca\nNaN\nPampamarca\nNaN\nNaN\nNaN\nPampamarca\nNaN\nNaN\nNaN\nNaN\n\n\n4\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPampamarca\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1714\nPampamarca\nNaN\nPampamarca\nNaN\nPampamarca\nNaN\nPampamarca\nNaN\nNaN\nNaN\nNaN\n\n\n1715\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1716\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1717\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1718\nPampamarca\nNaN\nPampamarca\nNaN\nPampamarca\nNaN\nPampamarca\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n1719 rows × 11 columns\n\n\n\n\nentierros_place_columns = [\n    'event_place', 'deceased_hometown', 'burial_place', 'event_geographic_descriptor_1',\n    'event_geographic_descriptor_2', 'event_geographic_descriptor_3',\n    'event_geographic_descriptor_4'\n]\n\nfor col in entierros_place_columns:\n    if col in ENTIERROS_HARMONIZED.columns:\n        ENTIERROS_HARMONIZED[col] = extractor.extract_places_per_row(ENTIERROS_HARMONIZED[col])\n\nENTIERROS_HARMONIZED[entierros_place_columns]\n\n\n\n\n\n\n\n\nevent_place\ndeceased_hometown\nburial_place\nevent_geographic_descriptor_1\nevent_geographic_descriptor_2\nevent_geographic_descriptor_3\nevent_geographic_descriptor_4\n\n\n\n\n0\nNaN\nNaN\nNaN\nNaN\nLucanas\nNaN\nNaN\n\n\n1\nNaN\nNaN\nNaN\nNaN\nLucanas\nNaN\nNaN\n\n\n2\nNaN\nNaN\nNaN\nNaN\nLucanas\nNaN\nNaN\n\n\n3\nNaN\nNaN\nNaN\nNaN\nLucanas\nNaN\nNaN\n\n\n4\nNaN\nNaN\nNaN\nNaN\nLucanas\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2193\nNaN\nSanta Ana de Aucara\nNaN\nNaN\nSanta Ana de Aucara\nNaN\nNaN\n\n\n2194\nNaN\nPampamarca\nNaN\nNaN\nPampamarca\nNaN\nNaN\n\n\n2195\nNaN\nSanta Ana de Aucara\nNaN\nNaN\nSanta Ana de Aucara\nNaN\nNaN\n\n\n2196\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2197\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n2198 rows × 7 columns\n\n\n\n\n%%capture\n\nbautismos_places = BAUTISMOS_HARMONIZED[bautismos_place_columns]\nmatrimonios_places = MATRIMONIOS_HARMONIZED[matrimonios_place_columns] \nentierros_places = ENTIERROS_HARMONIZED[entierros_place_columns]\n\nmap_places = placeRecognition.MapPlaces([bautismos_places, matrimonios_places, entierros_places], places_map=PLACES_MAP)\nall_unique_places = map_places.resolve_places()\nprint(\"All unique places extracted:\")\nprint(all_unique_places)\n\n\nall_unique_places.to_csv(\"../data/interim/unique_places.csv\", index=False)"
  },
  {
    "objectID": "2_placeMapping.html#manual-data",
    "href": "2_placeMapping.html#manual-data",
    "title": "Step 2: Create Unique Values from Data",
    "section": "Manual Data",
    "text": "Manual Data\nThe file data/interim/unique_places_manual.csv serves as the authoritative normalization reference for all place mentions across sacramental records. The resolution was performed manually to ensure accuracy for ambiguous toponyms such as Ishua and Pampamarca. The AuthoritativePlaceResolver class uses this file to link canonical names with their variants (mentioned_as) and resolves each canonical name using GeoNames, TGN, WHG, and Wikidata. The output (data/clean/unique_places.csv) is now the trusted lookup table for place resolution across the project.\nWe also added the category “administrative division” to places_types.json to be included in the geoResolver library, via placeRecognition module.\n\"administrative division\": {\n    \"geonames\": \"A\",\n    \"wikidata\": \"Q5\",\n    \"tgn\": \"administrative divisions\",\n    \"whg\": \"a\"\n  }\n\n%%capture\n\nmanual_data = '../data/interim/unique_places_manual.csv'\n\nmanual_data = pd.read_csv(manual_data)\n\nresolver = placeRecognition.AuthoritativePlaceResolver(data=manual_data, places_map=PLACES_MAP)\nresult_df = resolver.resolve_places()\n\nprint(\"Resolved places:\")\nprint(result_df)"
  },
  {
    "objectID": "2_placeMapping.html#add-place-ids",
    "href": "2_placeMapping.html#add-place-ids",
    "title": "Step 2: Create Unique Values from Data",
    "section": "Add Place IDs",
    "text": "Add Place IDs\nThis step is added in preparation for database integration, where each unique place will be assigned a unique identifier (place_id).\n\nresult_df['place_id'] = result_df.index + 1\nresult_df = result_df.set_index('place_id')\n\nresult_df\n\n\n\n\n\n\n\n\nmanually_normalized_place\nstandardize_label\nlanguage\nlatitude\nlongitude\nsource\nid\nuri\ncountry_code\npart_of\npart_of_uri\nconfidence\nthreshold\nmatch_type\nmentioned_as\n\n\nplace_id\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\nAcobamba\nAcobamba\nes\n-12.077570\n-74.871270\nGeoNames\n8663907\nhttp://sws.geonames.org/8663907/\nPE\n\n\n100.0\n90.0\nexact\n[Acobamba]\n\n\n2\nAlcamenca\nAlcamenca\nes\n-13.657049\n-74.147134\nWikidata\nQ130369762\nhttps://www.wikidata.org/entity/Q130369762\nPE\nDistrito de Alcamenca\nhttps://www.wikidata.org/entity/Q3824932\n100.0\n90.0\nexact\n[Alcamenca]\n\n\n4\nAndamarca\nAndamarca\nes\n-15.638330\n-70.588480\nGeoNames\n3947725\nhttp://sws.geonames.org/3947725/\nPE\n\n\n100.0\n90.0\nexact\n[Andamarca]\n\n\n5\nAntay\nAntayaje\nes\n-14.068861\n-71.755222\nWikidata\nQ97160355\nhttps://www.wikidata.org/entity/Q97160355\nPE\nDistrito de Omacha\nhttps://www.wikidata.org/entity/Q2893883\n100.0\n90.0\nexact\n[Antay]\n\n\n7\nAucará\nAucará\nes\n-14.250000\n-74.083330\nGeoNames\n3947087\nhttp://sws.geonames.org/3947087/\nPE\n\n\n100.0\n90.0\nexact\n[Aucara, Aucara Barrio de Mayo, Aucará, Barrio...\n\n\n11\nHuaycahuaycho\nHuaycahuaycho\nes\n-14.150000\n-74.016670\nGeoNames\n3939003\nhttp://sws.geonames.org/3939003/\nPE\n\n\n100.0\n90.0\nexact\n[Aucara Huaycahuacho, Huaicahuacho]\n\n\n12\nAyacucho\nAyacucho\nes\n-13.163800\n-74.223450\nGeoNames\n3947019\nhttp://sws.geonames.org/3947019/\nPE\n\n\n100.0\n90.0\nexact\n[Ayacucho, Ayacuchoi, Ciuda de Ayacucho, Ciuda...\n\n\n13\nCabana\nCabana\nes\n-8.400000\n-78.033330\nGeoNames\n3699185\nhttp://sws.geonames.org/3699185/\nPE\n\n\n100.0\n90.0\nexact\n[Aucara Cabana, Cabana]\n\n\n14\nCanaria\nCanaria\nes\n-13.816670\n-73.866670\nGeoNames\n3945879\nhttp://sws.geonames.org/3945879/\nPE\n\n\n100.0\n90.0\nexact\n[Canaria]\n\n\n15\nCangallo\nCangallo\nes\n-13.628610\n-74.143890\nGeoNames\n3945793\nhttp://sws.geonames.org/3945793/\nPE\n\n\n100.0\n90.0\nexact\n[Cangallo, provincia Cangallo]\n\n\n16\nCaralla\nCaralla\nes\n-15.295350\n-71.566920\nGeoNames\n3955233\nhttp://sws.geonames.org/3955233/\nPE\n\n\n100.0\n90.0\nexact\n[Caralla]\n\n\n17\nCarapo\nCarapo\nes\n-13.750000\n-74.233330\nGeoNames\n3945552\nhttp://sws.geonames.org/3945552/\nPE\n\n\n100.0\n90.0\nexact\n[Carapo]\n\n\n18\nCarmen Alto\nCarmen Alto\nes\n-14.700310\n-70.517300\nGeoNames\n13318788\nhttp://sws.geonames.org/13318788/\nPE\n\n\n100.0\n90.0\nexact\n[Carmen Alto]\n\n\n20\nCastilla Puquio\nCastilla Puquio\nes\n-15.900160\n-70.591520\nGeoNames\n3945238\nhttp://sws.geonames.org/3945238/\nPE\n\n\n100.0\n90.0\nexact\n[Castilla]\n\n\n21\nCayara\nCayara\nes\n-15.318360\n-73.550030\nGeoNames\n3945148\nhttp://sws.geonames.org/3945148/\nPE\n\n\n100.0\n90.0\nexact\n[Cayara]\n\n\n22\nChacralla\nChacralla\nes\n-14.207360\n-73.992780\nGeoNames\n3944632\nhttp://sws.geonames.org/3944632/\nPE\n\n\n100.0\n90.0\nexact\n[Ccaralla, Cementerio general Chacralla, Chac...\n\n\n23\nChecca\nChecca\nes\n-14.472780\n-71.393890\nWikidata\nQ5089024\nhttps://www.wikidata.org/entity/Q5089024\nPE\nDepartamento de Cusco\nhttps://www.wikidata.org/entity/Q205057\n100.0\n90.0\nexact\n[Ccecca, Cecca]\n\n\n24\nPampamarca\nPampamarca\nes\n-14.146390\n-71.459440\nGeoNames\n3933218\nhttp://sws.geonames.org/3933218/\nPE\n\n\n100.0\n90.0\nexact\n[Cementerio de Pampamarca, Cementerio general ...\n\n\n26\nChacalla\nChacalla\nes\n-13.968890\n-72.205560\nGeoNames\n6392254\nhttp://sws.geonames.org/6392254/\nPE\n\n\n100.0\n90.0\nexact\n[Chacalla]\n\n\n27\nChalhuanca\nChalhuanca\nes\n-14.300560\n-73.230830\nGeoNames\n3944545\nhttp://sws.geonames.org/3944545/\nPE\n\n\n100.0\n90.0\nexact\n[Chalhuanca, Challhuanca]\n\n\n28\nChilcas\nChilcas\nes\n-12.372840\n-74.846870\nGeoNames\n8664084\nhttp://sws.geonames.org/8664084/\nPE\n\n\n100.0\n90.0\nexact\n[Chilcas]\n\n\n29\nChumpe\nChumpe\nes\n-12.296110\n-75.697780\nGeoNames\n6304004\nhttp://sws.geonames.org/6304004/\nPE\n\n\n100.0\n90.0\nexact\n[Chumpe]\n\n\n30\nSan Nicolás\nPuerto San Nicolás\nes\n-15.261240\n-75.225360\nGeoNames\n3929384\nhttp://sws.geonames.org/3929384/\nPE\n\n\n100.0\n90.0\nexact\n[Colca]\n\n\n31\nCórdova\nPuesto Cabo Córdova\nes\n-4.191480\n-80.457730\nGeoNames\n3693175\nhttp://sws.geonames.org/3693175/\nPE\n\n\n100.0\n90.0\nexact\n[Cordoba]\n\n\n33\nHuacaña\nHuacaña\nes\n-15.518610\n-72.018840\nGeoNames\n3940068\nhttp://sws.geonames.org/3940068/\nPE\n\n\n100.0\n90.0\nexact\n[Huacaña]\n\n\n34\nHuallatalloc\nHuallatalloc\nes\n-13.516670\n-70.950000\nGeoNames\n3939794\nhttp://sws.geonames.org/3939794/\nPE\n\n\n100.0\n90.0\nexact\n[Hualla]\n\n\n35\nHuanca Santos\nHuanca Santos\nes\n-13.920000\n-74.334110\nGeoNames\n3939474\nhttp://sws.geonames.org/3939474/\nPE\n\n\n100.0\n90.0\nexact\n[Huanca, Sancos]\n\n\n36\nHuanta\nHuanta\nes\n-12.933330\n-74.250000\nGeoNames\n3939386\nhttp://sws.geonames.org/3939386/\nPE\n\n\n100.0\n90.0\nexact\n[Huanta]\n\n\n37\nSanta Ana de Huaycahuacho\nSanta Ana de Huaycahuacho\nes\n-14.950000\n-73.216670\nGeoNames\n3929281\nhttp://sws.geonames.org/3929281/\nPE\n\n\n100.0\n90.0\nexact\n[Huaycahuacho]\n\n\n38\nHuaytará\nHuaytará\nes\n-13.604450\n-75.353080\nGeoNames\n3938825\nhttp://sws.geonames.org/3938825/\nPE\n\n\n100.0\n90.0\nexact\n[Huaytara]\n\n\n39\nIca\nIca\nes\n-14.075380\n-75.734220\nGeoNames\n3938527\nhttp://sws.geonames.org/3938527/\nPE\n\n\n100.0\n70.0\nexact\n[Ica]\n\n\n40\nIshua\nIshua\nes\n-14.249860\n-73.955560\nGeoNames\n3938160\nhttp://sws.geonames.org/3938160/\nPE\n\n\n100.0\n90.0\nexact\n[Iglesia de Yshua, India de Hishua, Ishua, Yshua]\n\n\n41\nJauja\nJauja\nes\n-11.775840\n-75.496560\nGeoNames\n3937733\nhttp://sws.geonames.org/3937733/\nPE\n\n\n100.0\n90.0\nexact\n[Jauja]\n\n\n42\nJulcamarca\nJulcamarca\nes\n-9.288680\n-77.990510\nGeoNames\n6380634\nhttp://sws.geonames.org/6380634/\nPE\n\n\n100.0\n90.0\nexact\n[Julcamarca]\n\n\n43\nLaramate\nLaramate\nes\n-13.027150\n-74.387020\nGeoNames\n3936871\nhttp://sws.geonames.org/3936871/\nPE\n\n\n100.0\n90.0\nexact\n[Laramate]\n\n\n44\nLarcay\nLarcay\nes\n-12.738050\n-74.536730\nGeoNames\n8661432\nhttp://sws.geonames.org/8661432/\nPE\n\n\n100.0\n90.0\nexact\n[Larcay]\n\n\n45\nLima\nLima\nes\n-12.043180\n-77.028240\nGeoNames\n3936456\nhttp://sws.geonames.org/3936456/\nPE\n\n\n100.0\n70.0\nexact\n[Lima]\n\n\n47\nCajamarca\nCajamarca\nes\n-7.163780\n-78.500270\nGeoNames\n3699088\nhttp://sws.geonames.org/3699088/\nPE\n\n\n100.0\n90.0\nexact\n[Mar]\n\n\n48\nMayobamba\nMayobamba\nes\n-14.375830\n-73.891940\nGeoNames\n3935121\nhttp://sws.geonames.org/3935121/\nPE\n\n\n100.0\n90.0\nexact\n[Mayobamba]\n\n\n50\nMoyobamba\nMoyobamba\nes\n-6.034410\n-76.974230\nGeoNames\n3694564\nhttp://sws.geonames.org/3694564/\nPE\n\n\n100.0\n90.0\nexact\n[Moyobamba]\n\n\n51\nProvincia de Nazca\nProvincia de Nazca\nes\n-14.830980\n-74.938950\nGeoNames\n3934356\nhttp://sws.geonames.org/3934356/\nPE\n\n\n100.0\n90.0\nexact\n[Nasca, Nazca]\n\n\n52\nOcaña\nOcaña\nes\n-13.045060\n-74.378400\nGeoNames\n3934206\nhttp://sws.geonames.org/3934206/\nPE\n\n\n100.0\n90.0\nexact\n[Ocaña]\n\n\n53\nPampa Grande\nPampa Grande\nes\n-6.751690\n-79.481010\nGeoNames\n3693998\nhttp://sws.geonames.org/3693998/\nPE\n\n\n100.0\n90.0\nexact\n[Pampa]\n\n\n54\nPuno\nPuno\nes\n-15.840030\n-70.021980\nGeoNames\n3931276\nhttp://sws.geonames.org/3931276/\nPE\n\n\n100.0\n70.0\nexact\n[Puna]\n\n\n55\nPuquio\nPuquio\nes\n-14.700000\n-74.133330\nGeoNames\n3931223\nhttp://sws.geonames.org/3931223/\nPE\n\n\n100.0\n90.0\nexact\n[Puquio]\n\n\n56\nQuerobamba\nQuerobamba\nes\n-13.866670\n-73.833330\nGeoNames\n3930947\nhttp://sws.geonames.org/3930947/\nPE\n\n\n100.0\n90.0\nexact\n[Querobamba]\n\n\n57\nSacsamarca\nSacsamarca\nes\n-13.150850\n-74.278690\nGeoNames\n3930088\nhttp://sws.geonames.org/3930088/\nPE\n\n\n100.0\n90.0\nexact\n[Sacsamarca]\n\n\n58\nSan Jeronimo De Tunan\nSan Jeronimo De Tunan\nes\n-11.955910\n-75.284110\nGeoNames\n3929611\nhttp://sws.geonames.org/3929611/\nPE\n\n\n100.0\n90.0\nexact\n[San Jeronimo]\n\n\n59\nSan Juan\nSan Juan\nes\n-3.776110\n-73.281390\nGeoNames\n3692462\nhttp://sws.geonames.org/3692462/\nPE\n\n\n100.0\n90.0\nexact\n[San Juan de Lucanas]\n\n\n60\nSanta Ana\nSanta Ana\nes\n-12.866670\n-72.716670\nGeoNames\n3929295\nhttp://sws.geonames.org/3929295/\nPE\n\n\n100.0\n90.0\nexact\n[Santa Ana, Santa Ana de Aucara]\n\n\n61\nSondondo\nSondondo\nes\n-14.297950\n-73.939280\nGeoNames\n3928450\nhttp://sws.geonames.org/3928450/\nPE\n\n\n100.0\n90.0\nexact\n[Sondondo]\n\n\n62\nSoras Pata\nSoras Pata\nes\n-14.237410\n-70.650110\nGeoNames\n13238703\nhttp://sws.geonames.org/13238703/\nPE\n\n\n100.0\n90.0\nexact\n[Soras]\n\n\n63\nUmasi\nUmasi\nes\n-14.891420\n-70.687010\nGeoNames\n13238711\nhttp://sws.geonames.org/13238711/\nPE\n\n\n100.0\n90.0\nexact\n[Umaci, Umasi]\n\n\n64\nUrubamba\nUrubamba\nes\n-13.304720\n-72.115830\nGeoNames\n3926438\nhttp://sws.geonames.org/3926438/\nPE\n\n\n100.0\n90.0\nexact\n[Urabamba]\n\n\n65\nVilcashuamán\nVilcashuamán\nes\n-13.653610\n-73.953060\nGeoNames\n3926141\nhttp://sws.geonames.org/3926141/\nPE\n\n\n100.0\n90.0\nexact\n[Vilcas]\n\n\n66\nVilla San Juan\nVilla San Juan\nes\n-6.372520\n-79.802920\nGeoNames\n3820188\nhttp://sws.geonames.org/3820188/\nPE\n\n\n100.0\n90.0\nexact\n[Villa de San Juan]\n\n\n\n\n\n\n\n\nresult_df.to_csv(\"../data/clean/unique_places.csv\", index=True)"
  }
]